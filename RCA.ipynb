{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQVni6CaxLR5",
        "outputId": "0edaaea0-18e3-4952-857a-4ff1439ddbce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully: /content/downloads/OpenStack.tar.gz\n",
            "File is saved in the path /content/downloads/OpenStack.tar.gz\n",
            "File is saved in the path /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "def download_file_to_cwd(url, file_name=None):\n",
        "    try:\n",
        "        # Get the current working directory\n",
        "        current_dir = os.getcwd()\n",
        "\n",
        "        # create downloads directory\n",
        "        downloads_dir = os.path.join(current_dir, 'downloads')\n",
        "        os.makedirs(downloads_dir, exist_ok=True)\n",
        "\n",
        "        # Get the file name from the URL if not provided\n",
        "        if file_name is None:\n",
        "            file_name = url.split(\"/\")[-1].split(\"?\")[0]\n",
        "\n",
        "        # Full path to save the file\n",
        "        save_path = os.path.join(downloads_dir, file_name)\n",
        "\n",
        "        # Send a GET request to fetch the file\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()  # Raise an error for bad status codes\n",
        "\n",
        "        # Write the file to the specified directory\n",
        "        with open(save_path, 'wb') as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "\n",
        "        print(f\"File downloaded successfully: {save_path}\")\n",
        "        return current_dir,save_path\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading the file: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example Usage\n",
        "url = \"https://zenodo.org/records/8196385/files/OpenStack.tar.gz?download=1\"  # Replace with your URL\n",
        "current_dir,save_path=download_file_to_cwd(url)\n",
        "print(f\"File is saved in the path {save_path}\")\n",
        "print(f\"File is saved in the path {current_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT3ruCv_xLR8",
        "outputId": "f344171f-aa2e-4844-afa5-4b5b06c11160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_files\n",
            "Extraction completed.\n",
            "Extracted files: ['openstack_normal2.log', 'anomaly_labels.txt', 'openstack_abnormal.log', 'openstack_normal1.log']\n",
            "\n",
            "Number of records in each file:\n",
            "openstack_normal2.log: 137074 records\n",
            "anomaly_labels.txt: 6 records\n",
            "openstack_abnormal.log: 18434 records\n",
            "openstack_normal1.log: 52312 records\n"
          ]
        }
      ],
      "source": [
        "import tarfile\n",
        "import re\n",
        " # Get the file name from the URL if not provided\n",
        "extracted_files=\"extracted_files\"\n",
        "\n",
        "output_dir= os.path.join(current_dir, extracted_files)\n",
        "print(output_dir)\n",
        "\n",
        "with tarfile.open(save_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=output_dir)\n",
        "print(\"Extraction completed.\")\n",
        "\n",
        "# Step 3: List the contents of the extracted folder\n",
        "extracted_files = os.listdir(output_dir)\n",
        "print(\"Extracted files:\", extracted_files)\n",
        "\n",
        "# Step 4: Count the number of records in each file\n",
        "print(\"\\nNumber of records in each file:\")\n",
        "for file_name in extracted_files:\n",
        "    file_path = os.path.join(output_dir, file_name)\n",
        "    if os.path.isfile(file_path):  # Only process regular files\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                line_count = sum(1 for line in f)\n",
        "            print(f\"{file_name}: {line_count} records\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_name}: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW0oDqHsxLR9",
        "outputId": "ac9163c6-6a96-45bd-c105-5336afc9730a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "File Statistics:\n",
            "                file_name  line_count  file_size_kb\n",
            "0   openstack_normal2.log      137074      39557.00\n",
            "1      anomaly_labels.txt           6          0.24\n",
            "2  openstack_abnormal.log       18434       5308.71\n",
            "3   openstack_normal1.log       52312      15136.32\n",
            "\n",
            "Sample lines from openstack_normal2.log:\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:54.960 2931 INFO nova.compute.manager [req-7a738b84-d574-43c6-a6c4-68c164365101 e887c6de57b5411cb33a5943be2d3c1a 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 0f079bdd-4117-4f6a-8b49-f3fb720b483c] Took 0.54 seconds to deallocate network for instance.\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:55.746 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:55.747 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:55.748 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742\n",
            "nova-api.log.1.2017-05-17_12:02:19 2017-05-16 15:16:01.511 25749 INFO nova.osapi_compute.wsgi.server [req-378bb69b-363b-4c4f-a92c-a0e59baa5ca0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/flavors HTTP/1.1\" status: 200 len: 1581 time: 0.0648119\n",
            "\n",
            "Sample lines from anomaly_labels.txt:\n",
            "The following VM instances have injected anomalies as observed in openstack_abnormal.log.\n",
            "\n",
            "544fd51c-4edc-4780-baae-ba1d80a0acfc\n",
            "ae651dff-c7ad-43d6-ac96-bbcd820ccca8\n",
            "a445709b-6ad0-40ec-8860-bec60b6ca0c2\n",
            "\n",
            "Sample lines from openstack_abnormal.log:\n",
            "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:01.445 25746 INFO nova.osapi_compute.wsgi.server [req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1919448\n",
            "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:01.650 25746 INFO nova.osapi_compute.wsgi.server [req-c26a7d54-55ab-412e-947f-421a2cb934fc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/3edec1e4-9678-4a3a-a21b-a145a4ee5e61 HTTP/1.1\" status: 200 len: 1708 time: 0.2011580\n",
            "nova-compute.log.2017-05-14_21:27:09 2017-05-14 19:39:02.007 2931 INFO nova.virt.libvirt.driver [req-e285b551-587f-4c1d-8eba-dceb2673637f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 3edec1e4-9678-4a3a-a21b-a145a4ee5e61] Creating image\n",
            "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:02.924 25746 INFO nova.osapi_compute.wsgi.server [req-eb681812-78ae-4a9f-9e2a-96e505285512 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1759 time: 0.2698390\n",
            "nova-compute.log.2017-05-14_21:27:09 2017-05-14 19:39:03.166 2931 INFO nova.compute.manager [-] [instance: 2b590f10-49fd-4ec9-ae41-19596c2f4b25] VM Stopped (Lifecycle Event)\n",
            "\n",
            "Sample lines from openstack_normal1.log:\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:00.008 25746 INFO nova.osapi_compute.wsgi.server [req-38101a0b-2096-447d-96ea-a692162415ae 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2477829\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:00.272 25746 INFO nova.osapi_compute.wsgi.server [req-9bc36dd9-91c5-4314-898a-47625eb93b09 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2577181\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:01.551 25746 INFO nova.osapi_compute.wsgi.server [req-55db2d8d-cdb7-4b4b-993b-429be84c0c3e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2731631\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:01.813 25746 INFO nova.osapi_compute.wsgi.server [req-2a3dc421-6604-42a7-9390-a18dc824d5d6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2580249\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:03.091 25746 INFO nova.osapi_compute.wsgi.server [req-939eb332-c1c1-4e67-99b8-8695f8f1980a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2727931\n",
            "openstack_normal2.log\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# File statistics\n",
        "file_stats = []\n",
        "for file_name in extracted_files:\n",
        "    file_path = os.path.join(output_dir, file_name)\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            lines = f.readlines()\n",
        "            line_count = len(lines)\n",
        "            file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
        "            sample_lines = lines[:5]  # Inspect first 5 lines\n",
        "            file_stats.append({\n",
        "                \"file_name\": file_name,\n",
        "                \"line_count\": line_count,\n",
        "                \"file_size_kb\": round(file_size, 2),\n",
        "                \"sample_lines\": sample_lines\n",
        "            })\n",
        "df_stats = pd.DataFrame(file_stats)\n",
        "print(\"\\nFile Statistics:\")\n",
        "print(df_stats[[\"file_name\", \"line_count\", \"file_size_kb\"]])\n",
        "\n",
        "# Display sample lines from each file\n",
        "for index, row in df_stats.iterrows():\n",
        "    print(f\"\\nSample lines from {row['file_name']}:\")\n",
        "    for line in row[\"sample_lines\"]:\n",
        "        print(line.strip())\n",
        "\n",
        "#print(df_stats[\"file_name\"])\n",
        "# Analyze log components in the first file\n",
        "if not df_stats.empty:\n",
        "    filer_obj=[col for col in df_stats[\"file_name\"] if '_normal' in col]\n",
        "    print(filer_obj[0])\n",
        "    #first_file_path = os.path.join(output_dir, filer_obj)\n",
        "    #with open(first_file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "     #   logs = f.readlines()\n",
        "\n",
        "    # Extract log levels\n",
        "    #log_levels = [re.search(r'\\b(INFO|WARNING|ERROR|DEBUG|CRITICAL)\\b', log) for log in logs]\n",
        "    #log_levels = [match.group(0) for match in log_levels if match]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "QHVR3zf0xLR-",
        "outputId": "870d635e-53ac-4f93-8956-fb2ad06c5af8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'log_levels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-14b39ee43a71>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_levels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'log_levels' is not defined"
          ]
        }
      ],
      "source": [
        "print(log_levels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JNGAMN4FxLR-",
        "outputId": "c47ea3a6-208d-4777-b904-0fcedc676e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "File Statistics:\n",
            "                file_name  line_count  file_size_kb\n",
            "0   openstack_normal2.log      137074      39557.00\n",
            "1      anomaly_labels.txt           6          0.24\n",
            "2  openstack_abnormal.log       18434       5308.71\n",
            "3   openstack_normal1.log       52312      15136.32\n",
            "\n",
            "Sample lines from openstack_normal2.log:\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:54.960 2931 INFO nova.compute.manager [req-7a738b84-d574-43c6-a6c4-68c164365101 e887c6de57b5411cb33a5943be2d3c1a 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 0f079bdd-4117-4f6a-8b49-f3fb720b483c] Took 0.54 seconds to deallocate network for instance.\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:55.746 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:55.747 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:55.748 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742\n",
            "nova-api.log.1.2017-05-17_12:02:19 2017-05-16 15:16:01.511 25749 INFO nova.osapi_compute.wsgi.server [req-378bb69b-363b-4c4f-a92c-a0e59baa5ca0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/flavors HTTP/1.1\" status: 200 len: 1581 time: 0.0648119\n",
            "\n",
            "Sample lines from anomaly_labels.txt:\n",
            "The following VM instances have injected anomalies as observed in openstack_abnormal.log.\n",
            "\n",
            "544fd51c-4edc-4780-baae-ba1d80a0acfc\n",
            "ae651dff-c7ad-43d6-ac96-bbcd820ccca8\n",
            "a445709b-6ad0-40ec-8860-bec60b6ca0c2\n",
            "\n",
            "Sample lines from openstack_abnormal.log:\n",
            "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:01.445 25746 INFO nova.osapi_compute.wsgi.server [req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1919448\n",
            "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:01.650 25746 INFO nova.osapi_compute.wsgi.server [req-c26a7d54-55ab-412e-947f-421a2cb934fc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/3edec1e4-9678-4a3a-a21b-a145a4ee5e61 HTTP/1.1\" status: 200 len: 1708 time: 0.2011580\n",
            "nova-compute.log.2017-05-14_21:27:09 2017-05-14 19:39:02.007 2931 INFO nova.virt.libvirt.driver [req-e285b551-587f-4c1d-8eba-dceb2673637f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 3edec1e4-9678-4a3a-a21b-a145a4ee5e61] Creating image\n",
            "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:02.924 25746 INFO nova.osapi_compute.wsgi.server [req-eb681812-78ae-4a9f-9e2a-96e505285512 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1759 time: 0.2698390\n",
            "nova-compute.log.2017-05-14_21:27:09 2017-05-14 19:39:03.166 2931 INFO nova.compute.manager [-] [instance: 2b590f10-49fd-4ec9-ae41-19596c2f4b25] VM Stopped (Lifecycle Event)\n",
            "\n",
            "Sample lines from openstack_normal1.log:\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:00.008 25746 INFO nova.osapi_compute.wsgi.server [req-38101a0b-2096-447d-96ea-a692162415ae 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2477829\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:00.272 25746 INFO nova.osapi_compute.wsgi.server [req-9bc36dd9-91c5-4314-898a-47625eb93b09 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2577181\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:01.551 25746 INFO nova.osapi_compute.wsgi.server [req-55db2d8d-cdb7-4b4b-993b-429be84c0c3e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2731631\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:01.813 25746 INFO nova.osapi_compute.wsgi.server [req-2a3dc421-6604-42a7-9390-a18dc824d5d6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2580249\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:03.091 25746 INFO nova.osapi_compute.wsgi.server [req-939eb332-c1c1-4e67-99b8-8695f8f1980a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2727931\n",
            "\n",
            "Log Level Counts:\n",
            "INFO        134916\n",
            "WARNING       1988\n",
            "ERROR          169\n",
            "CRITICAL         1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Hourly Log Counts:\n",
            "hour\n",
            "0     8146\n",
            "1     8224\n",
            "2     8170\n",
            "3     8089\n",
            "4     8139\n",
            "5     8094\n",
            "6     4845\n",
            "7     2439\n",
            "8     2438\n",
            "9     2434\n",
            "10    2443\n",
            "11    2434\n",
            "12     110\n",
            "15    5985\n",
            "16    8176\n",
            "17    8123\n",
            "18    8059\n",
            "19    8107\n",
            "20    8154\n",
            "21    8115\n",
            "22    8181\n",
            "23    8169\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most Frequent Words:\n",
            "[('-', 361592), ('INFO', 134916), ('-]', 127082), ('2931', 71143), ('2017-05-16', 71074), ('54fadb412c4e40cdbaed9335e4c35a9e', 67688), ('113d3a99c3da401fbd62cc2caa5b96d2', 67687), ('2017-05-17', 66042), ('nova-api.log.1.2017-05-17_12:02:19', 65224), ('HTTP/1.1\"', 62693)]\n",
            "\n",
            "Entity Counts:\n",
            "Total UUIDs: 131089\n",
            "Total IP Addresses: 62693\n",
            "\n",
            "Service-Level Log Counts:\n",
            "nova: 137074\n",
            "neutron: 0\n",
            "cinder: 0\n",
            "keystone: 3\n",
            "glance: 0\n",
            "swift: 0\n",
            "heat: 0\n",
            "\n",
            "Total Anomalous Logs: 170\n",
            "\n",
            "Sample Anomalous Logs:\n",
            "['nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 16:30:35.360 2931 ERROR oslo_service.periodic_task [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Error during ComputeManager._run_image_cache_manager_pass\\n', 'nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 16:30:35.360 2931 ERROR oslo_service.periodic_task Traceback (most recent call last):\\n', 'nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 16:30:35.360 2931 ERROR oslo_service.periodic_task   File \"/usr/lib/python2.7/dist-packages/oslo_service/periodic_task.py\", line 220, in run_periodic_tasks\\n', 'nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 16:30:35.360 2931 ERROR oslo_service.periodic_task     task(self, context)\\n', 'nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 16:30:35.360 2931 ERROR oslo_service.periodic_task   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 6743, in _run_image_cache_manager_pass\\n']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# File statistics\n",
        "file_stats = []\n",
        "for file_name in extracted_files:\n",
        "    file_path = os.path.join(output_dir, file_name)\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            lines = f.readlines()\n",
        "            line_count = len(lines)\n",
        "            file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
        "            sample_lines = lines[:5]  # Inspect first 5 lines\n",
        "            file_stats.append({\n",
        "                \"file_name\": file_name,\n",
        "                \"line_count\": line_count,\n",
        "                \"file_size_kb\": round(file_size, 2),\n",
        "                \"sample_lines\": sample_lines\n",
        "            })\n",
        "df_stats = pd.DataFrame(file_stats)\n",
        "print(\"\\nFile Statistics:\")\n",
        "print(df_stats[[\"file_name\", \"line_count\", \"file_size_kb\"]])\n",
        "\n",
        "# Display sample lines from each file\n",
        "for index, row in df_stats.iterrows():\n",
        "    print(f\"\\nSample lines from {row['file_name']}:\")\n",
        "    for line in row[\"sample_lines\"]:\n",
        "        print(line.strip())\n",
        "\n",
        "# Analyze log components in the first file\n",
        "if not df_stats.empty:\n",
        "    filer_obj=[col for col in df_stats[\"file_name\"] if '_normal' in col]\n",
        "    first_file_path = os.path.join(output_dir, filer_obj[0])\n",
        "    with open(first_file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        logs = f.readlines()\n",
        "\n",
        "    # Extract log levels\n",
        "    log_levels = [re.search(r'\\b(INFO|WARNING|ERROR|DEBUG|CRITICAL)\\b', log) for log in logs]\n",
        "    log_levels = [match.group(0) for match in log_levels if match]\n",
        "\n",
        "    # Extract timestamps\n",
        "    datepattern = re.compile(\"\\w{3}, \\d{2} \\w{3} \\d{4} \\d{2}:\\d{2}:\\d{2} \\w{3}\")\n",
        "    timestamps = [re.search(r'\\b(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\b', log) for log in logs]\n",
        "    timestamps = [datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S') for match in timestamps if match]\n",
        "\n",
        "    # Log Level Counts\n",
        "    log_level_counts = pd.Series(log_levels).value_counts()\n",
        "    print(\"\\nLog Level Counts:\")\n",
        "    print(log_level_counts)\n",
        "\n",
        "    # Time-based patterns\n",
        "    df_timestamps = pd.DataFrame({'timestamp': timestamps})\n",
        "    df_timestamps['hour'] = df_timestamps['timestamp'].dt.hour\n",
        "    hourly_counts = df_timestamps['hour'].value_counts().sort_index()\n",
        "    print(\"\\nHourly Log Counts:\")\n",
        "    print(hourly_counts)\n",
        "\n",
        "    # Plot hourly log counts\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    hourly_counts.plot(kind='bar', color='skyblue', title='Hourly Log Counts')\n",
        "    plt.xlabel('Hour of Day')\n",
        "    plt.ylabel('Log Count')\n",
        "    plt.show()\n",
        "\n",
        "    # Frequent log messages\n",
        "    word_counts = Counter(\" \".join(logs).split())\n",
        "    print(\"\\nMost Frequent Words:\")\n",
        "    print(word_counts.most_common(10))\n",
        "\n",
        "    # Entity Extraction\n",
        "    uuids = [re.search(r'\\b[0-9a-fA-F-]{36}\\b', log).group(0) for log in logs if re.search(r'\\b[0-9a-fA-F-]{36}\\b', log)]\n",
        "    ip_addresses = [re.search(r'\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b', log).group(0) for log in logs if re.search(r'\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b', log)]\n",
        "    print(\"\\nEntity Counts:\")\n",
        "    print(f\"Total UUIDs: {len(uuids)}\")\n",
        "    print(f\"Total IP Addresses: {len(ip_addresses)}\")\n",
        "\n",
        "    # Service-Level Statistics\n",
        "    services = ['nova', 'neutron', 'cinder', 'keystone', 'glance', 'swift', 'heat']\n",
        "    service_counts = {service: sum(1 for log in logs if service in log) for service in services}\n",
        "    print(\"\\nService-Level Log Counts:\")\n",
        "    for service, count in service_counts.items():\n",
        "        print(f\"{service}: {count}\")\n",
        "\n",
        "    # Anomaly Detection\n",
        "    anomaly_keywords = [\"failed\", \"timeout\", \"critical\", \"error\"]\n",
        "    anomalous_logs = [log for log in logs if any(keyword in log.lower() for keyword in anomaly_keywords)]\n",
        "    print(f\"\\nTotal Anomalous Logs: {len(anomalous_logs)}\")\n",
        "    print(\"\\nSample Anomalous Logs:\")\n",
        "    print(anomalous_logs[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NjUKjyYOxLR_"
      },
      "outputs": [],
      "source": [
        "normal_log_filename1 = \"/content/extracted_files/openstack_normal1.log\"\n",
        "normal_log_filename2 = \"/content/extracted_files/openstack_normal2.log\"\n",
        "abnormal_log_filename = \"/content/extracted_files/openstack_abnormal.log\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "17jr5UodxLR_"
      },
      "outputs": [],
      "source": [
        "with open(normal_log_filename1, 'r') as file:\n",
        "    log_data = file.readlines()\n",
        "normal_logs1 = [line.strip() for line in log_data if line.strip()]\n",
        "\n",
        "with open(normal_log_filename2, 'r') as file:\n",
        "    log_data = file.readlines()\n",
        "normal_logs2 = [line.strip() for line in log_data if line.strip()]\n",
        "\n",
        "normal_logs = normal_logs1 +normal_logs2\n",
        "\n",
        "with open(abnormal_log_filename, 'r') as file:\n",
        "    log_data = file.readlines()\n",
        "abnormal_logs = [line.strip() for line in log_data if line.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YmuSwTCvxLR_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_and_normalize_log(logs):\n",
        "    cleaned_logs = []\n",
        "    log_line_pattern = r\"\"\" (?P<log_file>[\\w-]+\\.log\\.\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})\\s (?P<log_timestamp>\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\s (?P<pid>\\d+)\\s (?P<level>\\w+)\\s (?P<logger>[\\w\\.]+)\\s \\[(?P<req_id>req-[\\w-]+)\\s-\\s-\\s-\\s-\\s-\\] \\s (?P<message>.*) \"\"\"\n",
        "\n",
        "    for log in logs:\n",
        "        match = re.match(log_line_pattern, log.strip(), re.VERBOSE)\n",
        "        if match:\n",
        "          log_data = match.groupdict()\n",
        "          cleaned_logs.append(log_data.get(\"message\"))\n",
        "    return cleaned_logs\n",
        "\n",
        "normal = clean_and_normalize_log(normal_logs)\n",
        "abnormal = clean_and_normalize_log(abnormal_logs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_line_pattern = r\"\"\" (?P<log_file>[\\w-]+\\.log\\.\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})\\s (?P<log_timestamp>\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\s (?P<pid>\\d+)\\s (?P<level>\\w+)\\s (?P<logger>[\\w\\.]+)\\s \\[(?P<req_id>req-[\\w-]+)\\s-\\s-\\s-\\s-\\s-\\] \\s (?P<message>.*) \"\"\"\n",
        "log=abnormal_logs[:1]\n",
        "print(log[0])\n",
        "print(re.match(log_line_pattern,log[0].strip(),re.VERBOSE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo7DsyqlNlTQ",
        "outputId": "0869587c-548d-4086-8c44-3aaa40769c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:01.445 25746 INFO nova.osapi_compute.wsgi.server [req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1919448\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(normal))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xXnXytcRxZK",
        "outputId": "9755a464-60aa-4cb0-8213-db0fce3549e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e1lDP1fYOPsz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_and_normalize_log(logs):\n",
        "    cleaned_logs = []\n",
        "    log_line_pattern = r\"\"\" (?P<log_file>[\\w-]+\\.log\\.\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})\\s (?P<log_timestamp>\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\s (?P<pid>\\d+)\\s (?P<level>\\w+)\\s (?P<logger>[\\w\\.]+)\\s \\[(?P<req_id>req-[\\w-]+)\\s-\\s-\\s-\\s-\\s-\\] \\s (?P<message>.*) \"\"\"\n",
        "\n",
        "    for log in logs:\n",
        "        match = re.match(log_line_pattern, log.strip(), re.VERBOSE)\n",
        "        if match:\n",
        "          log_data = match.groupdict()\n",
        "          cleaned_logs.append(log_data.get(\"message\"))\n",
        "    return cleaned_logs\n",
        "\n",
        "normal = clean_and_normalize_log(normal_logs)\n",
        "abnormal = clean_and_normalize_log(abnormal_logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFq2hlbxxLSA",
        "outputId": "ecd5e913-f6b8-4519-e03e-1f827878b62f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 log  label\n",
            "0  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...      0\n",
            "1  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...      0\n",
            "2  Active base files: /var/lib/nova/instances/_ba...      0\n",
            "3  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...      0\n",
            "4  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...      0\n",
            "\n",
            "Label Counts:\n",
            "label\n",
            "0    13662\n",
            "1     5936\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Detailed Label Counts:\n",
            "Normal logs (label=0): 13662\n",
            "Abnormal logs (label=1): 5936\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data = []\n",
        "for log in normal:\n",
        "    data.append({\"log\": log.strip(), \"label\": 0})  # 0 for normal\n",
        "for log in abnormal:\n",
        "    data.append({\"log\": log.strip(), \"label\": 1})  # 1 for abnormal\n",
        "\n",
        "# Create DataFrame\n",
        "df_logs = pd.DataFrame(data)\n",
        "\n",
        "# Print the first few rows\n",
        "print(df_logs.head())\n",
        "\n",
        "# Generate label counts\n",
        "label_counts = df_logs['label'].value_counts()\n",
        "\n",
        "# Display label counts\n",
        "print(\"\\nLabel Counts:\")\n",
        "print(label_counts)\n",
        "\n",
        "# Optional: Label counts with descriptions\n",
        "print(\"\\nDetailed Label Counts:\")\n",
        "print(f\"Normal logs (label=0): {label_counts.get(0, 0)}\")\n",
        "print(f\"Abnormal logs (label=1): {label_counts.get(1, 0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install drain3 transformers torch scikit-learn pandas numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q9V2QL0y1bu",
        "outputId": "c2bfe36f-2083-4e42-a75f-b469c3b870bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting drain3\n",
            "  Downloading drain3-0.9.11.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting jsonpickle==1.5.1 (from drain3)\n",
            "  Downloading jsonpickle-1.5.1-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting cachetools==4.2.1 (from drain3)\n",
            "  Downloading cachetools-4.2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
            "Downloading jsonpickle-1.5.1-py2.py3-none-any.whl (37 kB)\n",
            "Building wheels for collected packages: drain3\n",
            "  Building wheel for drain3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for drain3: filename=drain3-0.9.11-py3-none-any.whl size=23998 sha256=333dc41dccb4b64e05f500854fdb19467dc54756888ddecbef2a6870ce0aeba7\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/3f/bb/c2df80298168b46a45654266ac0c139220540689a17463e3cf\n",
            "Successfully built drain3\n",
            "Installing collected packages: jsonpickle, cachetools, drain3\n",
            "  Attempting uninstall: jsonpickle\n",
            "    Found existing installation: jsonpickle 4.0.1\n",
            "    Uninstalling jsonpickle-4.0.1:\n",
            "      Successfully uninstalled jsonpickle-4.0.1\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.5.0\n",
            "    Uninstalling cachetools-5.5.0:\n",
            "      Successfully uninstalled cachetools-5.5.0\n",
            "Successfully installed cachetools-4.2.1 drain3-0.9.11 jsonpickle-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCrC897nxLSA",
        "outputId": "5f769af1-579d-4c9d-dbbc-9b17cc1c238b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 log  label  \\\n",
            "0  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...      0   \n",
            "1  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...      0   \n",
            "2  Active base files: /var/lib/nova/instances/_ba...      0   \n",
            "3  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...      0   \n",
            "4  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...      0   \n",
            "\n",
            "                                            template  template_id  \n",
            "0  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...            1  \n",
            "1  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...            2  \n",
            "2  Active base files: /var/lib/nova/instances/_ba...            3  \n",
            "3  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...            4  \n",
            "4  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...            5  \n"
          ]
        }
      ],
      "source": [
        "from drain3 import TemplateMiner\n",
        "from drain3.template_miner_config import TemplateMinerConfig\n",
        "\n",
        "# Initialize Drain\n",
        "config = TemplateMinerConfig()\n",
        "template_miner = TemplateMiner(config=config)\n",
        "\n",
        "# Parse logs and generate structured templates\n",
        "log_sequences = []\n",
        "template_ids = []\n",
        "\n",
        "for log in df_logs[\"log\"]:\n",
        "    result = template_miner.add_log_message(log)\n",
        "    log_sequences.append(result[\"template_mined\"])\n",
        "    template_ids.append(result[\"cluster_id\"])\n",
        "\n",
        "# Add parsed templates and IDs to DataFrame\n",
        "df_logs[\"template\"] = log_sequences\n",
        "df_logs[\"template_id\"] = template_ids\n",
        "df_logs.to_csv('log_structured.csv')\n",
        "print(df_logs.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LBEOcewOxLSB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert template IDs to sequences for LogBERT\n",
        "df_logs[\"sequence\"] = df_logs[\"template_id\"].astype(str)  # Use template IDs as sequence\n",
        "\n",
        "# Train-test split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df_logs[\"sequence\"].tolist(),\n",
        "    df_logs[\"label\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iP9vP1GcxLSB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "a70dfce9e4c84b3fa2d297be347d56e9",
            "5e99a3d55b7d4b65907d54b20238858b",
            "29c7e653b5b545f291bd6b276aea5878",
            "ecfbb65b9a4c420dac2eaf9e0603d154",
            "aeec9f29a06947ebb6252c99413addd2",
            "7beb8ca995414a0dbf7df8688d78fe9e",
            "94c972afdd504316a489e03ca6470504",
            "7688855c5a774272af78dafe745764f7",
            "19140dc1521f4d2b9044c4b05815a336",
            "5c53d5bc19be4b0d9518de41338bdeff",
            "b44ba3f101784e9cae124679939a7309",
            "07e23ab3021844969e895e4822cbf303",
            "176bd3e0b7b6432fb1c1d7c9c8d1c629",
            "42ea37e5520c431d8be84d67fc4af88e",
            "c25b7ec403d647f7938b9988dc4280af",
            "cfae9edd124c4958a598608f110a0ce5",
            "f6e36984f51a4978ad05a24d14f43a9a",
            "50e32412744844ee8b4bbe987fa149fc",
            "ff8fddc4767e4b6cb64467b988eaa11c",
            "4f872f106d51499d9e336e2d60e4977e",
            "4dee307c89b0462bb21654a4f1078f23",
            "f829a74211f44477859c4da1bc21c14f",
            "15d4b22c0b13445b8febbab4d277033e",
            "26a9cb545196440e84423d508cc24972",
            "d200fc70cd964810bd79858d260cfe5d",
            "4ce64bda7e1740f392c5079c4bb7f457",
            "32856705b5824d15a29510dec45a5462",
            "81afd0e4aac34a66aa7f23cb5e3c5e54",
            "9b895c0dffbb4ce88022e428562a4f72",
            "3cfd64d76ab44dfa971a288ee90f2335",
            "a345f9c5c8de412ca22965bbc372c515",
            "0a550078491c4eedbae4b5da3f1e8e34",
            "f507e0755f9d44908f57369da8e1fdea",
            "840071dbe09d4dd1a94de47a92899979",
            "b3e3aebd084b4dbaaa1db4728dd95cd6",
            "d2eefc03dd9f4aa5a444b5ac88238770",
            "0aab503274834e4db3dd58a723c7384b",
            "01939f3f71784da4ab4c3d770f187d9e",
            "0d5f24bbf14d414197d0ec854d28662d",
            "800a7f7b60e347adbba3ee6276faf6b2",
            "1f780d58bab74d7d8b03722e0e1dcbd6",
            "5e0090741fb14ef39f2438a9de08f050",
            "825dfbcbfa4e4226a19368faba8ddc9c",
            "e5dd7f0640804d1c9ba5d0d6632f4a2a"
          ]
        },
        "outputId": "12cf2c4d-e7ec-4c94-b2a6-7948534082f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a70dfce9e4c84b3fa2d297be347d56e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07e23ab3021844969e895e4822cbf303"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15d4b22c0b13445b8febbab4d277033e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "840071dbe09d4dd1a94de47a92899979"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "class LogDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LogDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = LogDataset(val_texts, val_labels, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwiPUpWYk9mQ",
        "outputId": "8684d030-6c70-4405-d663-ce714a03fd53"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 101, 1015,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'labels': tensor(0)}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Assume df_logs is your dataset\n",
        "# df_logs contains \"template_id\" and \"label\" columns.\n",
        "\n",
        "# Check initial distribution of labels\n",
        "print(\"Original label distribution:\")\n",
        "print(df_logs[\"label\"].value_counts())\n",
        "\n",
        "# Subsample 5,000 records from each label\n",
        "df_class_0 = df_logs[df_logs[\"label\"] == 0].sample(n=5000, random_state=42)\n",
        "df_class_1 = df_logs[df_logs[\"label\"] == 1].sample(n=5000, random_state=42)\n",
        "\n",
        "# Combine the two classes and shuffle\n",
        "df_balanced = pd.concat([df_class_0, df_class_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split the balanced data into train and test sets\n",
        "X = data[['log', 'template', 'template_id']]  # Features (assumed as template_id)\n",
        "y = df_balanced[\"label\"].values        # Labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Feature extraction: Convert template_id to numerical sequences\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()  # Bag of Words representation for simplicity\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# 1. Decision Tree Classifier\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree.fit(X_train_vec, y_train)\n",
        "y_pred_tree = decision_tree.predict(X_test_vec)\n",
        "\n",
        "print(\"\\nDecision Tree Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tree))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_tree))\n",
        "\n",
        "# 2. Logistic Regression\n",
        "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logistic_regression.fit(X_train_vec, y_train)\n",
        "y_pred_lr = logistic_regression.predict(X_test_vec)\n",
        "\n",
        "print(\"\\nLogistic Regression Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "pQNR_YwvkUxR",
        "outputId": "f6aa75b6-2296-4947-9f57-f3cf5acffba9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original label distribution:\n",
            "label\n",
            "0    13662\n",
            "1     5936\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-476c897e30e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Split the balanced data into train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'template'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'template_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Features (assumed as template_id)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_balanced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m        \u001b[0;31m# Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the CSV file\n",
        "data = df_logs\n",
        "\n",
        "# Extract features and target\n",
        "X = data['log']  # Features\n",
        "y = data['label']  # Target: The class labels (0 or 1)\n",
        "\n",
        "# Vectorizing 'log' and 'template' columns (text columns)\n",
        "vectorizer_log = TfidfVectorizer(stop_words='english')\n",
        "vectorizer_template = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "X_log_vect = vectorizer_log.fit_transform(X)\n",
        "#X_template_vect = vectorizer_template.fit_transform(X['template'])\n",
        "\n",
        "# Convert 'template_id' to a separate column, if it's not already numeric\n",
        "#X_template_id = X['template_id'].values.reshape(-1, 1)\n",
        "\n",
        "# Combining all the features: log, template, and template_id\n",
        "#from scipy.sparse import hstack\n",
        "#X_combined = hstack([X_log_vect, X_template_vect, X_template_id])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_log_vect, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate Decision Tree model\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "print(\"Decision Tree Classifier Accuracy:\")\n",
        "print(accuracy_score(y_test, dt_predictions))\n",
        "print(classification_report(y_test, dt_predictions))\n",
        "\n",
        "# Logistic Regression Classifier\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate Logistic Regression model\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy:\")\n",
        "print(accuracy_score(y_test, lr_predictions))\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "vectorizer_log = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit the vectorizer on the 'log' column of training data\n",
        "vectorizer_log.fit(data['log'])\n",
        "\n",
        "# New log message to predict\n",
        "new_log_message = \"Disk quota exceeded for user john_doe on server xyz123\"\n",
        "\n",
        "# Transform the new log message using the fitted vectorizer\n",
        "new_log_vect = vectorizer_log.transform([new_log_message])\n",
        "\n",
        "# Load the trained Decision Tree model (assuming it was trained previously)\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "# If saved model: dt_model = joblib.load('decision_tree_model.pkl')\n",
        "\n",
        "# Make a prediction using the trained model\n",
        "prediction = dt_model.predict(new_log_vect)\n",
        "\n",
        "# Display the prediction result (0 - Normal, 1 - Anomaly)\n",
        "print(\"Anomaly\" if prediction[0] == 1 else \"Normal\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "MgUr-g01o6lm",
        "outputId": "08ecd699-c706-4d13-9828-4ecbbd2489d7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier Accuracy:\n",
            "0.8564625850340136\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      1.00      0.91      4139\n",
            "           1       1.00      0.52      0.68      1741\n",
            "\n",
            "    accuracy                           0.86      5880\n",
            "   macro avg       0.92      0.76      0.79      5880\n",
            "weighted avg       0.88      0.86      0.84      5880\n",
            "\n",
            "Logistic Regression Accuracy:\n",
            "0.8554421768707483\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      1.00      0.91      4139\n",
            "           1       0.99      0.52      0.68      1741\n",
            "\n",
            "    accuracy                           0.86      5880\n",
            "   macro avg       0.91      0.76      0.79      5880\n",
            "weighted avg       0.88      0.86      0.84      5880\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-eecdf0cabfee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Make a prediction using the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_log_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Display the prediction result (0 - Normal, 1 - Anomaly)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \"\"\"\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load your DataFrame (replace this with your actual df_logs loading code)\n",
        "data = df_logs\n",
        "\n",
        "# Extract features and target\n",
        "X = data['log']  # Features (log messages)\n",
        "y = data['label']  # Target: The class labels (0 or 1)\n",
        "\n",
        "# Vectorizing 'log' column\n",
        "vectorizer_log = TfidfVectorizer(stop_words='english')\n",
        "X_log_vect = vectorizer_log.fit_transform(X)  # Fit and transform log messages\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_log_vect, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate Decision Tree model\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "print(\"Decision Tree Classifier Accuracy:\")\n",
        "print(accuracy_score(y_test, dt_predictions))\n",
        "print(classification_report(y_test, dt_predictions))\n",
        "\n",
        "# Logistic Regression Classifier\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate Logistic Regression model\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy:\")\n",
        "print(accuracy_score(y_test, lr_predictions))\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "\n",
        "# Now, let's predict using a new log message\n",
        "\n",
        "# New log message for testing\n",
        "new_log_message = \"Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=2560MB phys_disk=15GB used_disk=20GB total_vcpus=16 used_vcpus=1 pci_stats=[].\"\n",
        "\n",
        "# Transform the new log message using the fitted vectorizer (NO RE-FITTING, just transforming)\n",
        "new_log_vect = vectorizer_log.transform([new_log_message])\n",
        "\n",
        "# Make predictions using the Decision Tree model\n",
        "dt_prediction = dt_model.predict(new_log_vect)\n",
        "print(\"Decision Tree Prediction: Anomaly\" if dt_prediction[0] == 1 else \"Normal\")\n",
        "\n",
        "# Make predictions using the Logistic Regression model\n",
        "lr_prediction = lr_model.predict(new_log_vect)\n",
        "print(\"Logistic Regression Prediction: Anomaly\" if lr_prediction[0] == 1 else \"Normal\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN44vnOMyHU3",
        "outputId": "060f1aa1-e615-4c2f-a8b4-8dff460b34a6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier Accuracy:\n",
            "0.8564625850340136\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      1.00      0.91      4139\n",
            "           1       1.00      0.52      0.68      1741\n",
            "\n",
            "    accuracy                           0.86      5880\n",
            "   macro avg       0.92      0.76      0.79      5880\n",
            "weighted avg       0.88      0.86      0.84      5880\n",
            "\n",
            "Logistic Regression Accuracy:\n",
            "0.8554421768707483\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      1.00      0.91      4139\n",
            "           1       0.99      0.52      0.68      1741\n",
            "\n",
            "    accuracy                           0.86      5880\n",
            "   macro avg       0.91      0.76      0.79      5880\n",
            "weighted avg       0.88      0.86      0.84      5880\n",
            "\n",
            "Normal\n",
            "Normal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# After training the model and vectorizer\n",
        "joblib.dump(vectorizer_log, 'vectorizer_log.pkl')  # Save vectorizer\n",
        "joblib.dump(dt_model, 'decision_tree_model.pkl')  # Save the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7XjcpY1uYny",
        "outputId": "dd383499-9228-4299-8bf2-10c988aa3d67"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['decision_tree_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load previously saved vectorizer and model\n",
        "vectorizer_log = joblib.load('vectorizer_log.pkl')  # Load the saved TfidfVectorizer\n",
        "dt_model = joblib.load('decision_tree_model.pkl')  # Load the trained Decision Tree model\n",
        "\n",
        "# New log message\n",
        "new_log_message = \"Disk quota exceeded for user john_doe on server xyz123\"\n",
        "\n",
        "# Transform the new log message using the saved vectorizer\n",
        "new_log_vect = vectorizer_log.transform([new_log_message])\n",
        "\n",
        "# Make a prediction using the trained model\n",
        "prediction = dt_model.predict(new_log_vect)\n",
        "\n",
        "# Display the result\n",
        "print(\"Anomaly\" if prediction[0] == 1 else \"Normal\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "zHwDxfciuKzq",
        "outputId": "a835ca18-a6c3-44ff-e27c-ebc4b0e9b531"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "The TF-IDF vectorizer is not fitted",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-3fecfcf74da4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Transform the new log message using the saved vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnew_log_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_log_message\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Make a prediction using the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2124\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m         \"\"\"\n\u001b[0;32m-> 2126\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The TF-IDF vectorizer is not fitted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Assuming the model and vectorizers are saved previously (as part of your workflow)\n",
        "# If you're not able to load the saved vectorizer and model, fit them again on the training data\n",
        "\n",
        "# Load your training data (replace with your actual CSV)\n",
        "data = df_logs\n",
        "\n",
        "# Vectorizing 'log' from training data\n",
        "vectorizer_log = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit the vectorizer on the 'log' column of training data\n",
        "vectorizer_log.fit(data['log'])\n",
        "\n",
        "# New log message to predict\n",
        "new_log_message = \"Error: Instance 0673dd71-34c5-4fbb-86c4-40623fbe45b4 failed to launch due to insufficient storage.\"\n",
        "\n",
        "# Transform the new log message using the fitted vectorizer\n",
        "new_log_vect = vectorizer_log.transform([new_log_message])\n",
        "\n",
        "# Load the trained Decision Tree model (assuming it was trained previously)\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "# If saved model: dt_model = joblib.load('decision_tree_model.pkl')\n",
        "\n",
        "# Make a prediction using the trained model\n",
        "prediction = dt_model.predict(new_log_vect)\n",
        "\n",
        "# Display the prediction result (0 - Normal, 1 - Anomaly)\n",
        "print(\"Anomaly\" if prediction[0] == 1 else \"Normal\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "wGIU1p0nqTx5",
        "outputId": "7b26c1d9-223a-4986-8f05-a34640af7017"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-33bae5307c9f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Make a prediction using the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_log_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Display the prediction result (0 - Normal, 1 - Anomaly)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \"\"\"\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "25c55ed764ff4029bc366cf4b171ac83",
            "3ddb6bde36784389a85e0f8ebcbedaff",
            "e0b4379952214f6daab7fc5ebe083510",
            "a1d1588883804c38bcd33a8501748e1f",
            "e7af2a55d33b4062b4136de70b2e388f",
            "ea2f64a2f95d4d21bf5c436b21a2577f",
            "19288a32bdb34ca4bfcafbdb64b17f5b",
            "165995cc5e5746568eca7d29b11b571a",
            "45f58fddebc546cea89222d25bb9fc4b",
            "7eeeb82409f6420c9e2a00a64009f009",
            "ccb743afdef24be7abc36e43742d3ace"
          ]
        },
        "id": "_RyIEXvMxLSC",
        "outputId": "e82e02b9-52d2-4fd6-b8ff-5c75243b56cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25c55ed764ff4029bc366cf4b171ac83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ceb0ccbe99ec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Define LogDataset class (same as before)\n",
        "class LogDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "df_logs[\"sequence\"] = df_logs[\"template_id\"].astype(str)  # Use template IDs as sequence\n",
        "sequences = df_logs[\"sequence\"].tolist()\n",
        "labels = df_logs[\"label\"].tolist()\n",
        "\n",
        "# Cross-Validation Loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(sequences, labels)):\n",
        "    print(f\"===== Fold {fold + 1} =====\")\n",
        "    train_texts = [sequences[i] for i in train_idx]\n",
        "    val_texts = [sequences[i] for i in val_idx]\n",
        "    train_labels = [labels[i] for i in train_idx]\n",
        "    val_labels = [labels[i] for i in val_idx]\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = LogDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = LogDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "\n",
        "    # Reset optimizer and model for each fold\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 3\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation performance\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"Fold {fold + 1}, Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "7SSyONbuYsDl",
        "outputId": "e379af36-3cbf-4fe4-d41b-2c50f61f9faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Fold 1 =====\n",
            "Fold 1, Epoch 1/3, Loss: 0.4240\n",
            "Fold 1, Epoch 2/3, Loss: 0.3975\n",
            "Fold 1, Epoch 3/3, Loss: 0.3980\n",
            "Fold 1, Validation Accuracy: 0.8625\n",
            "===== Fold 2 =====\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 80 is out of bounds for dimension 0 with size 80",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-b153e5f4a32b>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtrain_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mval_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-b153e5f4a32b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtrain_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mval_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 80 is out of bounds for dimension 0 with size 80"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Define LogDataset class\n",
        "class LogDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Preprocess data\n",
        "df_logs = df_logs.reset_index(drop=True)  # Ensure indices are consistent\n",
        "df_logs[\"sequence\"] = df_logs[\"template_id\"].astype(str)  # Use template IDs as sequences\n",
        "\n",
        "sequences = df_logs[\"sequence\"].tolist()\n",
        "labels = df_logs[\"label\"].tolist()\n",
        "\n",
        "# Ensure consistency in data\n",
        "assert len(sequences) == len(labels), \"Mismatch between sequences and labels.\"\n",
        "\n",
        "# Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(sequences, labels)):\n",
        "    print(f\"===== Fold {fold + 1} =====\")\n",
        "\n",
        "    # Split data into training and validation\n",
        "    train_texts = [sequences[i] for i in train_idx]\n",
        "    val_texts = [sequences[i] for i in val_idx]\n",
        "    train_labels = [labels[i] for i in train_idx]\n",
        "    val_labels = [labels[i] for i in val_idx]\n",
        "\n",
        "    print(f\"Fold {fold + 1}: {len(train_texts)} training examples, {len(val_texts)} validation examples.\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = LogDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = LogDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "\n",
        "    # Reset optimizer for each fold\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 3\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation performance\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"Fold {fold + 1}, Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "Rdl3jKkloPU-",
        "outputId": "7fbcce68-a9e8-43f2-b754-aea9b514539f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Fold 1 =====\n",
            "Fold 1: 15678 training examples, 3920 validation examples.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-d40ccaf0f9da>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Define LogDataset class\n",
        "class LogDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Preprocess data\n",
        "df_logs = df_logs.reset_index(drop=True)  # Ensure indices are consistent\n",
        "df_logs[\"sequence\"] = df_logs[\"template_id\"].astype(str)  # Use template IDs as sequences\n",
        "\n",
        "sequences = df_logs[\"sequence\"].tolist()\n",
        "labels = df_logs[\"label\"].tolist()\n",
        "\n",
        "# Ensure consistency in data\n",
        "assert len(sequences) == len(labels), \"Mismatch between sequences and labels.\"\n",
        "\n",
        "# Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(sequences, labels)):\n",
        "    print(f\"===== Fold {fold + 1} =====\")\n",
        "\n",
        "    # Split data into training and validation\n",
        "    train_texts = [sequences[i] for i in train_idx]\n",
        "    val_texts = [sequences[i] for i in val_idx]\n",
        "    train_labels = [labels[i] for i in train_idx]\n",
        "    val_labels = [labels[i] for i in val_idx]\n",
        "\n",
        "    print(f\"Fold {fold + 1}: {len(train_texts)} training examples, {len(val_texts)} validation examples.\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = LogDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = LogDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "\n",
        "    # Reset optimizer for each fold\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 3\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation performance\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"Fold {fold + 1}, Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "ELWzwI8zv_JQ",
        "outputId": "5386dafe-edb2-40af-99c9-f3cec692a087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Fold 1 =====\n",
            "Fold 1: 15678 training examples, 3920 validation examples.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 21.06 MiB is free. Process 2328 has 14.72 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 684.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-d40ccaf0f9da>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 )\n\u001b[1;32m    630\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    632\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     return (\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m     )\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 21.06 MiB is free. Process 2328 has 14.72 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 684.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer, get_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize model, tokenizer, and device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
        "\n",
        "# Scheduler\n",
        "num_training_steps = len(train_loader) * 5  # Assuming 5 epochs\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training configurations\n",
        "epochs = 5  # Increased number of epochs\n",
        "scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
        "\n",
        "# Training and Validation Loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision training\n",
        "        with torch.cuda.amp.autocast():\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass with gradient clipping\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "                logits = outputs.logits\n",
        "                predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"Validation Performance:\")\n",
        "    print(classification_report(true_labels, predictions, target_names=[\"Normal\", \"Anomalous\"]))\n",
        "\n",
        "# Save Model\n",
        "model.save_pretrained(\"fine_tuned_bert_model\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_bert_model\")\n",
        "\n",
        "print(\"Training completed and model saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJWdIKeDP-mj",
        "outputId": "66afff64-09da-4e14-e071-a0364558046d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-26-c6c27e89d3f0>:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
            "<ipython-input-26-c6c27e89d3f0>:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Training Loss: 0.4138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-c6c27e89d3f0>:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.4037\n",
            "Validation Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.83      1.00      0.90      2732\n",
            "   Anomalous       1.00      0.51      0.68      1188\n",
            "\n",
            "    accuracy                           0.85      3920\n",
            "   macro avg       0.91      0.76      0.79      3920\n",
            "weighted avg       0.88      0.85      0.84      3920\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-c6c27e89d3f0>:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-c6c27e89d3f0>:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.3943\n",
            "Validation Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.83      1.00      0.90      2732\n",
            "   Anomalous       1.00      0.51      0.68      1188\n",
            "\n",
            "    accuracy                           0.85      3920\n",
            "   macro avg       0.91      0.76      0.79      3920\n",
            "weighted avg       0.88      0.85      0.84      3920\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-c6c27e89d3f0>:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-c6c27e89d3f0>:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.3927\n",
            "Validation Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.83      1.00      0.90      2732\n",
            "   Anomalous       1.00      0.51      0.68      1188\n",
            "\n",
            "    accuracy                           0.85      3920\n",
            "   macro avg       0.91      0.76      0.79      3920\n",
            "weighted avg       0.88      0.85      0.84      3920\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-c6c27e89d3f0>:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-c6c27e89d3f0>:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.3937\n",
            "Validation Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.83      1.00      0.90      2732\n",
            "   Anomalous       1.00      0.51      0.68      1188\n",
            "\n",
            "    accuracy                           0.85      3920\n",
            "   macro avg       0.91      0.76      0.79      3920\n",
            "weighted avg       0.88      0.85      0.84      3920\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-c6c27e89d3f0>:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-c6c27e89d3f0>:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.3934\n",
            "Validation Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.83      1.00      0.90      2732\n",
            "   Anomalous       1.00      0.51      0.68      1188\n",
            "\n",
            "    accuracy                           0.85      3920\n",
            "   macro avg       0.91      0.76      0.79      3920\n",
            "weighted avg       0.88      0.85      0.84      3920\n",
            "\n",
            "Training completed and model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7fis-A4okQK",
        "outputId": "df1df09b-4179-4c6c-af7d-59caf045bccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from torch.optim import AdamW\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter suggestions\n",
        "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-4)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n",
        "\n",
        "    # Model and DataLoader with suggested hyperparameters\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(3):  # Fixed epoch count for tuning\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate validation loss\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    return avg_val_loss\n",
        "\n",
        "# Run hyperparameter search\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "print(\"Best trial:\", study.best_trial)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "fEhjbEueofxm",
        "outputId": "b6ce2709-d789-4271-c442-e50909260940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-17 09:40:19,913] A new study created in memory with name: no-name-443f4375-fa13-420a-bdb1-219189b7d97b\n",
            "<ipython-input-88-5739d7f1a45f>:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-4)\n",
            "<ipython-input-88-5739d7f1a45f>:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-01-17 09:56:09,402] Trial 0 finished with value: 0.6123055455636005 and parameters: {'lr': 1.060730908607851e-05, 'batch_size': 16, 'weight_decay': 0.0002930439924885171}. Best is trial 0 with value: 0.6123055455636005.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[W 2025-01-17 10:07:20,242] Trial 1 failed with parameters: {'lr': 7.145224986738668e-05, 'batch_size': 16, 'weight_decay': 1.978130946432064e-05} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-88-5739d7f1a45f>\", line 21, in objective\n",
            "    input_ids = batch[\"input_ids\"].to(device)\n",
            "KeyboardInterrupt\n",
            "[W 2025-01-17 10:07:20,244] Trial 1 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-5739d7f1a45f>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Run hyperparameter search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best trial:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-5739d7f1a45f>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gEKsTdjxLSC",
        "outputId": "8d65db40-0fd0-48a1-8370-bc0e6a2ea205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "g4q_7rr3xLSD",
        "outputId": "d0e9fce7-6c99-4014-c4c2-8f45fd1d0170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 1.0000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1188]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b2d78e862ffa>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Classification Report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mclass_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Normal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Anomalous\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nClassification Report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_report\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2691\u001b[0m             )\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2694\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(all_labels, all_preds, target_names=[\"Normal\", \"Anomalous\"])\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Anomalous\"], yticklabels=[\"Normal\", \"Anomalous\"])\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model.save_pretrained(\"log_anomaly_detection_model\")\n",
        "tokenizer.save_pretrained(\"log_anomaly_detection_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxZ6BczC5Edw",
        "outputId": "42707516-17d0-4662-db0b-443ec01b2a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('log_anomaly_detection_model/tokenizer_config.json',\n",
              " 'log_anomaly_detection_model/special_tokens_map.json',\n",
              " 'log_anomaly_detection_model/vocab.txt',\n",
              " 'log_anomaly_detection_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Function to predict whether a log sequence is anomalous or normal\n",
        "def predict(sequence, model, tokenizer):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(\n",
        "        sequence,  # Sequence should be a single string\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=tokens['input_ids'],\n",
        "            attention_mask=tokens['attention_mask']\n",
        "        )\n",
        "        probabilities = F.softmax(outputs.logits, dim=1)\n",
        "        prediction = torch.argmax(probabilities, dim=1).item()\n",
        "    return prediction\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"log_anomaly_detection_model\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"log_anomaly_detection_model\")\n",
        "\n",
        "# Test log sequences\n",
        "test_normal_log_sequence = (\n",
        "    \"image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): \"\n",
        "    \"in use: on this node 1 local, 0 on other nodes sharing this instance storage\"\n",
        ")\n",
        "prediction = predict(test_normal_log_sequence, model, tokenizer)\n",
        "print(\"Anomaly\" if prediction == 1 else \"Normal\")\n",
        "\n",
        "test_anomalous_log_sequence = (\n",
        "    \"Error: Instance 0673dd71-34c5-4fbb-86c4-40623fbe45b4 failed to launch due to insufficient storage.\"\n",
        ")\n",
        "prediction = predict(test_anomalous_log_sequence, model, tokenizer)\n",
        "print(\"Anomaly\" if prediction == 1 else \"Normal\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks8zrQuB5L2m",
        "outputId": "8e2ab60b-5147-467c-b6f6-ff0e0ecd22ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal\n",
            "Normal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_2L_j-m5eHD",
        "outputId": "e34a9d18-0a08-47a5-8430-d1b47c2dbc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.11.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.57.4\n",
            "    Uninstalling openai-1.57.4:\n",
            "      Successfully uninstalled openai-1.57.4\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize OpenAI API\n",
        "OPENAI_API_KEY = userdata.get('OA_API')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not openai.api_key:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please configure the API key.\")\n",
        "\n",
        "# Function to predict whether a log sequence is anomalous or normal\n",
        "def predict(sequence, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Predicts if the given log sequence is Anomalous (1) or Normal (0).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer(\n",
        "        sequence,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=tokens['input_ids'],\n",
        "            attention_mask=tokens['attention_mask']\n",
        "        )\n",
        "        probabilities = F.softmax(outputs.logits, dim=1)\n",
        "        prediction = torch.argmax(probabilities, dim=1).item()\n",
        "    return prediction\n",
        "\n",
        "# Function to perform root cause analysis using OpenAI LLM\n",
        "def root_cause_analysis(log_message):\n",
        "    \"\"\"\n",
        "    Sends the anomaly log to OpenAI LLM for a detailed RCA.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert in IT operations and log analysis. Analyze the following log anomaly and provide a detailed root cause analysis:\n",
        "\n",
        "        Log anomaly:\n",
        "        {log_message}\n",
        "\n",
        "        Root Cause Analysis:\n",
        "        \"\"\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a root cause analysis expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0.2,  # Low temperature for consistent output\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error during RCA generation: {e}\"\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"log_anomaly_detection_model\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"log_anomaly_detection_model\")\n",
        "\n",
        "# Test log sequence\n",
        "test_log_sequence = (\n",
        "    \"Error: Instance 0673dd71-34c5-4fbb-86c4-40623fbe45b4 failed to launch due to insufficient storage.\"\n",
        ")\n",
        "\n",
        "# Predict if the log is anomalous\n",
        "prediction = predict(test_log_sequence, model, tokenizer)\n",
        "if prediction == 1:  # Anomalous\n",
        "    print(\"Prediction: Anomaly detected.\")\n",
        "    # Perform RCA for the anomaly\n",
        "    rca_result = root_cause_analysis(test_log_sequence)\n",
        "    print(\"Detailed RCA:\\n\", rca_result)\n",
        "else:\n",
        "    print(\"Prediction: Normal log.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "K5V2s3zG5o-h",
        "outputId": "e4c9adb0-fbdd-4412-bc1f-55756c2638e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "__enter__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9acfdb2f21b7>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Predict if the log is anomalous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_log_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Anomalous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction: Anomaly detected.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-9acfdb2f21b7>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(sequence, model, tokenizer)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         outputs = model(\n\u001b[1;32m     30\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: __enter__"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDbrkw2UxLSD"
      },
      "source": [
        "# After effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq_5LyJGTJ5I",
        "outputId": "5ed7f070-3cfa-437a-847f-806f2654497c"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'D:\\\\project\\\\code_repo\\\\content\\\\OpenStack\\\\OpenStack.tar.gz'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m tar_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcode_repo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOpenStack\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOpenStack.tar.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m     14\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\project\\\\code_repo\\\\content\\\\OpenStack\\\\OpenStack.tar.gz'"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# URL of the TAR file\n",
        "url = \"https://zenodo.org/records/8196385/files/OpenStack.tar.gz?download=1\"\n",
        "output_dir = \"D:\\project\\\\code_repo\\\\content\"\n",
        "\n",
        "# Step 1: Download the TAR file\n",
        "tar_path = \"D:\\\\project\\\\code_repo\\\\content\\\\OpenStack\\\\OpenStack.tar.gz\"\n",
        "response = requests.get(url, stream=True)\n",
        "with open(tar_path, \"wb\") as file:\n",
        "    for chunk in response.iter_content(chunk_size=1024):\n",
        "        file.write(chunk)\n",
        "print(\"Download completed.\")\n",
        "\n",
        "# Step 2: Extract the TAR file\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=output_dir)\n",
        "print(\"Extraction completed.\")\n",
        "\n",
        "# Step 3: List the contents of the extracted folder\n",
        "extracted_files = os.listdir(output_dir)\n",
        "print(\"Extracted files:\", extracted_files)\n",
        "\n",
        "# Step 4: Count the number of records in each file\n",
        "print(\"\\nNumber of records in each file:\")\n",
        "for file_name in extracted_files:\n",
        "    file_path = os.path.join(output_dir, file_name)\n",
        "    if os.path.isfile(file_path):  # Only process regular files\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                line_count = sum(1 for line in f)\n",
        "            print(f\"{file_name}: {line_count} records\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_name}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C_ngMrx8A4kD",
        "outputId": "53acee43-6e00-424e-b6f2-db87c3a0e1c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download completed.\n",
            "Extraction completed.\n",
            "Extracted files: ['openstack_normal1.log', 'openstack_normal2.log', 'openstack_abnormal.log', 'anomaly_labels.txt']\n",
            "\n",
            "File Statistics:\n",
            "                file_name  line_count  file_size_kb\n",
            "0   openstack_normal1.log       52312      15136.32\n",
            "1   openstack_normal2.log      137074      39557.00\n",
            "2  openstack_abnormal.log       18434       5308.71\n",
            "3      anomaly_labels.txt           6          0.24\n",
            "\n",
            "Sample lines from openstack_normal1.log:\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:00.008 25746 INFO nova.osapi_compute.wsgi.server [req-38101a0b-2096-447d-96ea-a692162415ae 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2477829\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:00.272 25746 INFO nova.osapi_compute.wsgi.server [req-9bc36dd9-91c5-4314-898a-47625eb93b09 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2577181\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:01.551 25746 INFO nova.osapi_compute.wsgi.server [req-55db2d8d-cdb7-4b4b-993b-429be84c0c3e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2731631\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:01.813 25746 INFO nova.osapi_compute.wsgi.server [req-2a3dc421-6604-42a7-9390-a18dc824d5d6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2580249\n",
            "nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:03.091 25746 INFO nova.osapi_compute.wsgi.server [req-939eb332-c1c1-4e67-99b8-8695f8f1980a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2727931\n",
            "\n",
            "Sample lines from openstack_normal2.log:\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:54.960 2931 INFO nova.compute.manager [req-7a738b84-d574-43c6-a6c4-68c164365101 e887c6de57b5411cb33a5943be2d3c1a 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 0f079bdd-4117-4f6a-8b49-f3fb720b483c] Took 0.54 seconds to deallocate network for instance.\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:55.746 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:55.747 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742\n",
            "nova-compute.log.1.2017-05-17_12:02:35 2017-05-16 15:15:55.748 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742\n",
            "nova-api.log.1.2017-05-17_12:02:19 2017-05-16 15:16:01.511 25749 INFO nova.osapi_compute.wsgi.server [req-378bb69b-363b-4c4f-a92c-a0e59baa5ca0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/flavors HTTP/1.1\" status: 200 len: 1581 time: 0.0648119\n",
            "\n",
            "Sample lines from openstack_abnormal.log:\n",
            "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:01.445 25746 INFO nova.osapi_compute.wsgi.server [req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1919448\n",
            "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:01.650 25746 INFO nova.osapi_compute.wsgi.server [req-c26a7d54-55ab-412e-947f-421a2cb934fc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/3edec1e4-9678-4a3a-a21b-a145a4ee5e61 HTTP/1.1\" status: 200 len: 1708 time: 0.2011580\n",
            "nova-compute.log.2017-05-14_21:27:09 2017-05-14 19:39:02.007 2931 INFO nova.virt.libvirt.driver [req-e285b551-587f-4c1d-8eba-dceb2673637f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 3edec1e4-9678-4a3a-a21b-a145a4ee5e61] Creating image\n",
            "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:02.924 25746 INFO nova.osapi_compute.wsgi.server [req-eb681812-78ae-4a9f-9e2a-96e505285512 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1759 time: 0.2698390\n",
            "nova-compute.log.2017-05-14_21:27:09 2017-05-14 19:39:03.166 2931 INFO nova.compute.manager [-] [instance: 2b590f10-49fd-4ec9-ae41-19596c2f4b25] VM Stopped (Lifecycle Event)\n",
            "\n",
            "Sample lines from anomaly_labels.txt:\n",
            "The following VM instances have injected anomalies as observed in openstack_abnormal.log.\n",
            "\n",
            "544fd51c-4edc-4780-baae-ba1d80a0acfc\n",
            "ae651dff-c7ad-43d6-ac96-bbcd820ccca8\n",
            "a445709b-6ad0-40ec-8860-bec60b6ca0c2\n",
            "\n",
            "Log Level Counts:\n",
            "INFO        51431\n",
            "WARNING       855\n",
            "ERROR          25\n",
            "CRITICAL        1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Hourly Log Counts:\n",
            "hour\n",
            "0    8070\n",
            "1    8208\n",
            "2    8160\n",
            "3    8201\n",
            "4    8144\n",
            "5    8158\n",
            "6    3371\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Most Frequent Words:\n",
            "[('-', 130234), ('2017-05-16', 52326), ('INFO', 51431), ('-]', 48097), ('113d3a99c3da401fbd62cc2caa5b96d2', 28700), ('54fadb412c4e40cdbaed9335e4c35a9e', 28700), ('nova-api.log.1.2017-05-16_13:53:08', 27739), ('HTTP/1.1\"', 26624), ('status:', 26624), ('len:', 26624)]\n",
            "\n",
            "Entity Counts:\n",
            "Total UUIDs: 49786\n",
            "Total IP Addresses: 26624\n",
            "\n",
            "Service-Level Log Counts:\n",
            "nova: 52312\n",
            "neutron: 0\n",
            "cinder: 0\n",
            "keystone: 3\n",
            "glance: 0\n",
            "swift: 0\n",
            "heat: 0\n",
            "\n",
            "Total Anomalous Logs: 26\n",
            "\n",
            "Sample Anomalous Logs:\n",
            "['nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 03:19:45.356 2931 ERROR oslo_service.periodic_task [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Error during ComputeManager._run_image_cache_manager_pass\\n', 'nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 03:19:45.356 2931 ERROR oslo_service.periodic_task Traceback (most recent call last):\\n', 'nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 03:19:45.356 2931 ERROR oslo_service.periodic_task   File \"/usr/lib/python2.7/dist-packages/oslo_service/periodic_task.py\", line 220, in run_periodic_tasks\\n', 'nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 03:19:45.356 2931 ERROR oslo_service.periodic_task     task(self, context)\\n', 'nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 03:19:45.356 2931 ERROR oslo_service.periodic_task   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 6743, in _run_image_cache_manager_pass\\n']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Download and extract the TAR file\n",
        "url = \"https://zenodo.org/records/8196385/files/OpenStack.tar.gz?download=1\"\n",
        "output_dir = \"/content/sample_data/OpenStack\"\n",
        "tar_path = \"/content/OpenStack/OpenStack.tar.gz\"\n",
        "\n",
        "# Download TAR file\n",
        "response = requests.get(url, stream=True)\n",
        "os.makedirs(\"/content/OpenStack\", exist_ok=True)\n",
        "with open(tar_path, \"wb\") as file:\n",
        "    for chunk in response.iter_content(chunk_size=1024):\n",
        "        file.write(chunk)\n",
        "print(\"Download completed.\")\n",
        "\n",
        "# Extract TAR file\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=output_dir)\n",
        "print(\"Extraction completed.\")\n",
        "\n",
        "# List extracted files\n",
        "extracted_files = os.listdir(output_dir)\n",
        "print(\"Extracted files:\", extracted_files)\n",
        "\n",
        "# File statistics\n",
        "file_stats = []\n",
        "for file_name in extracted_files:\n",
        "    file_path = os.path.join(output_dir, file_name)\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            lines = f.readlines()\n",
        "            line_count = len(lines)\n",
        "            file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
        "            sample_lines = lines[:5]  # Inspect first 5 lines\n",
        "            file_stats.append({\n",
        "                \"file_name\": file_name,\n",
        "                \"line_count\": line_count,\n",
        "                \"file_size_kb\": round(file_size, 2),\n",
        "                \"sample_lines\": sample_lines\n",
        "            })\n",
        "df_stats = pd.DataFrame(file_stats)\n",
        "print(\"\\nFile Statistics:\")\n",
        "print(df_stats[[\"file_name\", \"line_count\", \"file_size_kb\"]])\n",
        "\n",
        "# Display sample lines from each file\n",
        "for index, row in df_stats.iterrows():\n",
        "    print(f\"\\nSample lines from {row['file_name']}:\")\n",
        "    for line in row[\"sample_lines\"]:\n",
        "        print(line.strip())\n",
        "\n",
        "# Analyze log components in the first file\n",
        "if not df_stats.empty:\n",
        "    first_file_path = os.path.join(output_dir, df_stats.iloc[0][\"file_name\"])\n",
        "    with open(first_file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        logs = f.readlines()\n",
        "\n",
        "    # Extract log levels\n",
        "    log_levels = [re.search(r'\\b(INFO|WARNING|ERROR|DEBUG|CRITICAL)\\b', log) for log in logs]\n",
        "    log_levels = [match.group(0) for match in log_levels if match]\n",
        "\n",
        "    # Extract timestamps\n",
        "    timestamps = [re.search(r'\\b(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\b', log) for log in logs]\n",
        "    timestamps = [datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S') for match in timestamps if match]\n",
        "\n",
        "    # Log Level Counts\n",
        "    log_level_counts = pd.Series(log_levels).value_counts()\n",
        "    print(\"\\nLog Level Counts:\")\n",
        "    print(log_level_counts)\n",
        "\n",
        "    # Time-based patterns\n",
        "    df_timestamps = pd.DataFrame({'timestamp': timestamps})\n",
        "    df_timestamps['hour'] = df_timestamps['timestamp'].dt.hour\n",
        "    hourly_counts = df_timestamps['hour'].value_counts().sort_index()\n",
        "    print(\"\\nHourly Log Counts:\")\n",
        "    print(hourly_counts)\n",
        "\n",
        "    # Plot hourly log counts\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    hourly_counts.plot(kind='bar', color='skyblue', title='Hourly Log Counts')\n",
        "    plt.xlabel('Hour of Day')\n",
        "    plt.ylabel('Log Count')\n",
        "    plt.show()\n",
        "\n",
        "    # Frequent log messages\n",
        "    word_counts = Counter(\" \".join(logs).split())\n",
        "    print(\"\\nMost Frequent Words:\")\n",
        "    print(word_counts.most_common(10))\n",
        "\n",
        "    # Entity Extraction\n",
        "    uuids = [re.search(r'\\b[0-9a-fA-F-]{36}\\b', log).group(0) for log in logs if re.search(r'\\b[0-9a-fA-F-]{36}\\b', log)]\n",
        "    ip_addresses = [re.search(r'\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b', log).group(0) for log in logs if re.search(r'\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b', log)]\n",
        "    print(\"\\nEntity Counts:\")\n",
        "    print(f\"Total UUIDs: {len(uuids)}\")\n",
        "    print(f\"Total IP Addresses: {len(ip_addresses)}\")\n",
        "\n",
        "    # Service-Level Statistics\n",
        "    services = ['nova', 'neutron', 'cinder', 'keystone', 'glance', 'swift', 'heat']\n",
        "    service_counts = {service: sum(1 for log in logs if service in log) for service in services}\n",
        "    print(\"\\nService-Level Log Counts:\")\n",
        "    for service, count in service_counts.items():\n",
        "        print(f\"{service}: {count}\")\n",
        "\n",
        "    # Anomaly Detection\n",
        "    anomaly_keywords = [\"failed\", \"timeout\", \"critical\", \"error\"]\n",
        "    anomalous_logs = [log for log in logs if any(keyword in log.lower() for keyword in anomaly_keywords)]\n",
        "    print(f\"\\nTotal Anomalous Logs: {len(anomalous_logs)}\")\n",
        "    print(\"\\nSample Anomalous Logs:\")\n",
        "    print(anomalous_logs[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cT7FVOOVkeK"
      },
      "outputs": [],
      "source": [
        "normal_log_filename1 = \"/content/sample_data/OpenStack/openstack_normal1.log\"\n",
        "normal_log_filename2 = \"/content/sample_data/OpenStack/openstack_normal2.log\"\n",
        "abnormal_log_filename = \"/content/sample_data/OpenStack/openstack_abnormal.log\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIvcK9IQTKHu"
      },
      "outputs": [],
      "source": [
        "with open(normal_log_filename1, 'r') as file:\n",
        "    log_data = file.readlines()\n",
        "normal_logs1 = [line.strip() for line in log_data if line.strip()]\n",
        "\n",
        "with open(normal_log_filename2, 'r') as file:\n",
        "    log_data = file.readlines()\n",
        "normal_logs2 = [line.strip() for line in log_data if line.strip()]\n",
        "\n",
        "normal_logs = normal_logs1 + normal_logs2\n",
        "\n",
        "with open(abnormal_log_filename, 'r') as file:\n",
        "    log_data = file.readlines()\n",
        "abnormal_logs = [line.strip() for line in log_data if line.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjt-3zZpl3Zc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_and_normalize_log(logs):\n",
        "    cleaned_logs = []\n",
        "    log_line_pattern = r\"\"\" (?P<log_file>[\\w-]+\\.log\\.\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})\\s (?P<log_timestamp>\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\s (?P<pid>\\d+)\\s (?P<level>\\w+)\\s (?P<logger>[\\w\\.]+)\\s \\[(?P<req_id>req-[\\w-]+)\\s-\\s-\\s-\\s-\\s-\\] \\s (?P<message>.*) \"\"\"\n",
        "\n",
        "    for log in logs:\n",
        "        match = re.match(log_line_pattern, log.strip(), re.VERBOSE)\n",
        "        if match:\n",
        "          log_data = match.groupdict()\n",
        "          cleaned_logs.append(log_data.get(\"message\"))\n",
        "    return cleaned_logs\n",
        "\n",
        "normal = clean_and_normalize_log(normal_logs)\n",
        "abnormal = clean_and_normalize_log(abnormal_logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGTn8dnKig68",
        "outputId": "a8fffe3d-33cb-407f-d696-bfc6c22af6a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                 log  label\n",
            "0  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...      0\n",
            "1  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...      0\n",
            "2  Active base files: /var/lib/nova/instances/_ba...      0\n",
            "3  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...      0\n",
            "4  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...      0\n",
            "\n",
            "Label Counts:\n",
            "label\n",
            "0    13662\n",
            "1     5936\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Detailed Label Counts:\n",
            "Normal logs (label=0): 13662\n",
            "Abnormal logs (label=1): 5936\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "for log in normal:\n",
        "    data.append({\"log\": log.strip(), \"label\": 0})  # 0 for normal\n",
        "for log in abnormal:\n",
        "    data.append({\"log\": log.strip(), \"label\": 1})  # 1 for abnormal\n",
        "\n",
        "# Create DataFrame\n",
        "df_logs = pd.DataFrame(data)\n",
        "\n",
        "# Print the first few rows\n",
        "print(df_logs.head())\n",
        "\n",
        "# Generate label counts\n",
        "label_counts = df_logs['label'].value_counts()\n",
        "\n",
        "# Display label counts\n",
        "print(\"\\nLabel Counts:\")\n",
        "print(label_counts)\n",
        "\n",
        "# Optional: Label counts with descriptions\n",
        "print(\"\\nDetailed Label Counts:\")\n",
        "print(f\"Normal logs (label=0): {label_counts.get(0, 0)}\")\n",
        "print(f\"Abnormal logs (label=1): {label_counts.get(1, 0)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8ENm-S1izHI",
        "outputId": "fd0bc6a2-762d-47ea-e4e4-998d272ef479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                 log  label  \\\n",
            "0  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...      0   \n",
            "1  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...      0   \n",
            "2  Active base files: /var/lib/nova/instances/_ba...      0   \n",
            "3  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...      0   \n",
            "4  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...      0   \n",
            "\n",
            "                                            template  template_id  \n",
            "0  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...            1  \n",
            "1  image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at ...            2  \n",
            "2  Active base files: /var/lib/nova/instances/_ba...            3  \n",
            "3  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...            4  \n",
            "4  [instance: 96c58299-fc8f-4be0-942e-8bcf0943176...            5  \n"
          ]
        }
      ],
      "source": [
        "from drain3 import TemplateMiner\n",
        "from drain3.template_miner_config import TemplateMinerConfig\n",
        "\n",
        "# Initialize Drain\n",
        "config = TemplateMinerConfig()\n",
        "template_miner = TemplateMiner(config=config)\n",
        "\n",
        "# Parse logs and generate structured templates\n",
        "log_sequences = []\n",
        "template_ids = []\n",
        "\n",
        "for log in df_logs[\"log\"]:\n",
        "    result = template_miner.add_log_message(log)\n",
        "    log_sequences.append(result[\"template_mined\"])\n",
        "    template_ids.append(result[\"cluster_id\"])\n",
        "\n",
        "# Add parsed templates and IDs to DataFrame\n",
        "df_logs[\"template\"] = log_sequences\n",
        "df_logs[\"template_id\"] = template_ids\n",
        "\n",
        "print(df_logs.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rrfrML7gavu"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert template IDs to sequences for LogBERT\n",
        "df_logs[\"sequence\"] = df_logs[\"template_id\"].astype(str)  # Use template IDs as sequence\n",
        "\n",
        "# Train-test split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df_logs[\"sequence\"].tolist(),\n",
        "    df_logs[\"label\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "7fe643368b0347c5aaf54c71e5c8c4bb",
            "6d969c4ce3294c56a873d21edeb1883a",
            "91a1a4c4198249728cf22e41ffb19588",
            "6b776697fa2e4a60ac2058cecea702c1",
            "96f3ad2491784c8eb11ce33a64bc443a",
            "01ef45b8f7574ad48706920800423434",
            "9156788c77c344758a6322209b52a51a",
            "98aaddb74e5d4bed8f4ba38592ba47d7",
            "03e69e2ffe5f418aaf6359bf28da48fc",
            "7e5af0cd54da4f6e8758e683e6a4f071",
            "344d77509b1b43abb5d51e4470a2f2cd",
            "8836c4e22fcc41e482f5864060b13d95",
            "8e2f79be2f1540bc8124653c8203a7f8",
            "85b42f3d80f04f878fd69649735631f4",
            "1d2135fadfe44fdebe27b0d94a26642f",
            "9e82cb8729be45ca9c0b2c83dea47a79",
            "0bc2140b288f42cc8a8ef1a65049f146",
            "3b3dc400472c48a898702c1cb4551ac7",
            "696497f3ea10479ba76ae7b1d7a83016",
            "5c44ae145ad94586a0967299c9819fb1",
            "844e9a9960064aa29fa32b434bd95623",
            "094e7e2cb59b49ac99bf4b1444b437be",
            "5dab3148b6dc4699a1480fb79351c0dd",
            "bc3697b1fe2941719d401be215b7c25b",
            "4168a05fb83440efbdb665b754ca0cde",
            "4c16fdeb6b134c23977a3cb87b24e5e8",
            "a10327ff462b41b3b1ba873db3a4640e",
            "494ff03411f4435eb9271b3fc5fa6deb",
            "72918e70644a45e9abfc1dc219a2c2d4",
            "32a2a750cda6483084c6c4efdafeb27e",
            "c6d2e80697f94902bf2923887deabd86",
            "0c986cf99be440d992ff1f08437cd5de",
            "c30c9d3142d94e5bb148f70d26ae571b",
            "c00f523fe5614671acb278bc2b6dff79",
            "be49729795de4015bfc3c5b1b9a542af",
            "9538e28ce0bb4c09b0515fce94ba32e8",
            "3128f91cacce42ca8f04f28a185f0d24",
            "2a32de61dd354dcf9ab425e3158bce68",
            "e8193f6d172949e6ba6cd807a46cb868",
            "cbd3ab61ad564cb7847f7095e0cde00a",
            "e441d6f7b6d645a0bb5a588f347adbe4",
            "428eee7a4e244b709dfbf5e40ffdc3dc",
            "c8a22ae328d34cd590def9f2f284659a",
            "f50af58621514b49a4aed340c1644c7f"
          ]
        },
        "id": "W5bziHhYhQi4",
        "outputId": "58f3b19b-b3c3-444e-bbfa-ac71d644dd9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fe643368b0347c5aaf54c71e5c8c4bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8836c4e22fcc41e482f5864060b13d95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dab3148b6dc4699a1480fb79351c0dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c00f523fe5614671acb278bc2b6dff79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "class LogDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LogDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = LogDataset(val_texts, val_labels, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "2392b709b19748bcb6f83f8b5f198be0",
            "ff7bcd027aac4b889566de4ed0c1fc4a",
            "8ca2a05d25c847fc90b1ec6ce7181bd9",
            "5062a729b31343488f875e6eeea2cc6b",
            "befe81fa10ae4b2a967e75aa00a36183",
            "1e86be31803744efb4baed2d0140774c",
            "b96afe25cbe845d089d65df6c76d8211",
            "f6dcdbf8c5514e38bc320a4f066f70b1",
            "924fd71d32074e90b6ed02f2160bee59",
            "dbe0e78d7a6e43ee803048847a8ddd66",
            "37dc3e6a7eb9408199b4605d4ae31e9d"
          ]
        },
        "id": "KaNlEKQrOYD9",
        "outputId": "1f038bf6-a4cc-4667-892a-37981c21125e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2392b709b19748bcb6f83f8b5f198be0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, Loss: 0.4082\n",
            "Epoch 2/3, Loss: 0.3912\n",
            "Epoch 3/3, Loss: 0.3911\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKtA2eczQKvA",
        "outputId": "400e50ec-b439-4964-c292-6d256edcb50c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8520\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "jZ7c_mJ-uVLZ",
        "outputId": "8cfa4ae6-6be4-4148-abe0-f5cb8f5f2b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8520\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2729    3]\n",
            " [ 577  611]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.83      1.00      0.90      2732\n",
            "   Anomalous       1.00      0.51      0.68      1188\n",
            "\n",
            "    accuracy                           0.85      3920\n",
            "   macro avg       0.91      0.76      0.79      3920\n",
            "weighted avg       0.88      0.85      0.84      3920\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(all_labels, all_preds, target_names=[\"Normal\", \"Anomalous\"])\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Anomalous\"], yticklabels=[\"Normal\", \"Anomalous\"])\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_POvVcsjhqC",
        "outputId": "cb9f9ff6-99d1-4edd-bd3f-2225918f7eeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('log_anomaly_detection_model/tokenizer_config.json',\n",
              " 'log_anomaly_detection_model/special_tokens_map.json',\n",
              " 'log_anomaly_detection_model/vocab.txt',\n",
              " 'log_anomaly_detection_model/added_tokens.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"log_anomaly_detection_model\")\n",
        "tokenizer.save_pretrained(\"log_anomaly_detection_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6ARJ3EOQ_Qp",
        "outputId": "ece53f4b-a65f-4a4d-866e-9e2af75b82c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normal\n",
            "Normal\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Function to predict whether a log sequence is anomalous or normal\n",
        "def predict(sequence, model, tokenizer):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(\n",
        "        sequence,  # Sequence should be a single string\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=tokens['input_ids'],\n",
        "            attention_mask=tokens['attention_mask']\n",
        "        )\n",
        "        probabilities = F.softmax(outputs.logits, dim=1)\n",
        "        prediction = torch.argmax(probabilities, dim=1).item()\n",
        "    return prediction\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"log_anomaly_detection_model\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"log_anomaly_detection_model\")\n",
        "\n",
        "# Test log sequences\n",
        "test_normal_log_sequence = (\n",
        "    \"image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): \"\n",
        "    \"in use: on this node 1 local, 0 on other nodes sharing this instance storage\"\n",
        ")\n",
        "prediction = predict(test_normal_log_sequence, model, tokenizer)\n",
        "print(\"Anomaly\" if prediction == 1 else \"Normal\")\n",
        "\n",
        "test_anomalous_log_sequence = (\n",
        "    \"Error: Instance 0673dd71-34c5-4fbb-86c4-40623fbe45b4 failed to launch due to insufficient storage.\"\n",
        ")\n",
        "prediction = predict(test_anomalous_log_sequence, model, tokenizer)\n",
        "print(\"Anomaly\" if prediction == 1 else \"Normal\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VOP7xuPSgL6-",
        "outputId": "822a18a6-a1cf-4a88-d37f-da7eab85da85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.4\n",
            "    Uninstalling openai-1.54.4:\n",
            "      Successfully uninstalled openai-1.54.4\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffd5LBgLfjyi",
        "outputId": "3284f3a8-6389-41ea-e753-296692a4bf49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: Normal log.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize OpenAI API\n",
        "OPENAI_API_KEY = userdata.get('OA_API')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not openai.api_key:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please configure the API key.\")\n",
        "\n",
        "# Function to predict whether a log sequence is anomalous or normal\n",
        "def predict(sequence, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Predicts if the given log sequence is Anomalous (1) or Normal (0).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer(\n",
        "        sequence,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=tokens['input_ids'],\n",
        "            attention_mask=tokens['attention_mask']\n",
        "        )\n",
        "        probabilities = F.softmax(outputs.logits, dim=1)\n",
        "        prediction = torch.argmax(probabilities, dim=1).item()\n",
        "    return prediction\n",
        "\n",
        "# Function to perform root cause analysis using OpenAI LLM\n",
        "def root_cause_analysis(log_message):\n",
        "    \"\"\"\n",
        "    Sends the anomaly log to OpenAI LLM for a detailed RCA.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert in IT operations and log analysis. Analyze the following log anomaly and provide a detailed root cause analysis:\n",
        "\n",
        "        Log anomaly:\n",
        "        {log_message}\n",
        "\n",
        "        Root Cause Analysis:\n",
        "        \"\"\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a root cause analysis expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0.2,  # Low temperature for consistent output\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error during RCA generation: {e}\"\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"log_anomaly_detection_model\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"log_anomaly_detection_model\")\n",
        "\n",
        "# Test log sequence\n",
        "test_log_sequence = (\n",
        "    \"Error: Instance 0673dd71-34c5-4fbb-86c4-40623fbe45b4 failed to launch due to insufficient storage.\"\n",
        ")\n",
        "\n",
        "# Predict if the log is anomalous\n",
        "prediction = predict(test_log_sequence, model, tokenizer)\n",
        "if prediction == 1:  # Anomalous\n",
        "    print(\"Prediction: Anomaly detected.\")\n",
        "    # Perform RCA for the anomaly\n",
        "    rca_result = root_cause_analysis(test_log_sequence)\n",
        "    print(\"Detailed RCA:\\n\", rca_result)\n",
        "else:\n",
        "    print(\"Prediction: Normal log.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "b-Ry5jKohado",
        "outputId": "8cabada7-4fbb-474c-82e2-a5b94e26db50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.7.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.5.0 (from gradio)\n",
            "  Downloading gradio_client-1.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.0->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.5.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.7.1-py3-none-any.whl (57.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.0-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.8.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.5 ffmpy-0.4.0 gradio-5.7.1 gradio-client-1.5.0 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.8.1 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.12.0 uvicorn-0.32.1 websockets-12.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "acf96247b1e2470e89e5ee3ce6b9f395",
              "pip_warning": {
                "packages": [
                  "markupsafe"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "pjq30-p0hXEC",
        "outputId": "e1976a46-f506-4dbf-b333-6e834f9d3e6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7abf03479d44fcbdca.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://7abf03479d44fcbdca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import openai\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "# Initialize OpenAI API\n",
        "OPENAI_API_KEY = userdata.get('OA_API')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not openai.api_key:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please configure the API key.\") # Replace with your OpenAI API key\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"log_anomaly_detection_model\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"log_anomaly_detection_model\")\n",
        "\n",
        "# Function to predict whether a log sequence is anomalous or normal\n",
        "def predict(sequence, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Predicts if the given log sequence is Anomalous (1) or Normal (0).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer(\n",
        "        sequence,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=tokens['input_ids'],\n",
        "            attention_mask=tokens['attention_mask']\n",
        "        )\n",
        "        probabilities = F.softmax(outputs.logits, dim=1)\n",
        "        prediction = torch.argmax(probabilities, dim=1).item()\n",
        "    return prediction\n",
        "\n",
        "# Function to perform root cause analysis using OpenAI LLM\n",
        "def root_cause_analysis(log_message):\n",
        "    \"\"\"\n",
        "    Sends the anomaly log to OpenAI LLM for a detailed RCA.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert in IT operations and log analysis. Analyze the following log anomaly and provide a detailed root cause analysis:\n",
        "\n",
        "        Log anomaly:\n",
        "        {log_message}\n",
        "\n",
        "        Root Cause Analysis:\n",
        "        \"\"\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a root cause analysis expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0.2,  # Low temperature for consistent output\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error during RCA generation: {e}\"\n",
        "\n",
        "# Gradio interface function\n",
        "def analyze_log(log_message):\n",
        "    \"\"\"\n",
        "    Classifies the log as normal or anomaly and generates RCA for anomalies.\n",
        "    \"\"\"\n",
        "    prediction = predict(log_message, model, tokenizer)\n",
        "    if prediction == 1:  # Anomalous\n",
        "        rca_result = root_cause_analysis(log_message)\n",
        "        return \"Anomaly Detected\", rca_result\n",
        "    else:\n",
        "        return \"Normal Log\", \"No RCA required for normal logs.\"\n",
        "\n",
        "# Create Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### Log Analysis and RCA\")\n",
        "\n",
        "    with gr.Row():\n",
        "        log_input = gr.Textbox(label=\"Log Message\", placeholder=\"Enter a log message\", lines=3)\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_button = gr.Button(\"Analyze Log\")\n",
        "\n",
        "    with gr.Row():\n",
        "        prediction_output = gr.Textbox(label=\"Log Classification\", interactive=False)\n",
        "        rca_output = gr.Textbox(label=\"Root Cause Analysis\", interactive=False)\n",
        "\n",
        "    # Connect function to Gradio components\n",
        "    submit_button.click(analyze_log, inputs=log_input, outputs=[prediction_output, rca_output])\n",
        "\n",
        "# Launch the app\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "aD70MT-slDWm",
        "outputId": "0cb104a5-46fd-449d-ae77-7a89971c5f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0ed6fa7e5f4d4cc88f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://0ed6fa7e5f4d4cc88f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import openai\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Initialize OpenAI API\n",
        "OPENAI_API_KEY = userdata.get('OA_API')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not openai.api_key:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please configure the API key.\") # Replace with your OpenAI API key\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"log_anomaly_detection_model\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"log_anomaly_detection_model\")\n",
        "\n",
        "# Function to predict whether a log sequence is anomalous or normal\n",
        "def predict(sequence, model, tokenizer, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Predicts if the given log sequence is Anomalous (1) or Normal (0) using a confidence threshold.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer(\n",
        "        sequence,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=tokens['input_ids'],\n",
        "            attention_mask=tokens['attention_mask']\n",
        "        )\n",
        "        probabilities = F.softmax(outputs.logits, dim=1)\n",
        "        anomaly_score = probabilities[0][1].item()  # Probability of anomaly (class 1)\n",
        "        print(f\"Anomaly Score: {anomaly_score}\")  # Debugging\n",
        "\n",
        "        if anomaly_score >= threshold:\n",
        "            return 1  # Anomalous\n",
        "        else:\n",
        "            return 0  # Normal\n",
        "\n",
        "# Function to perform root cause analysis using OpenAI LLM\n",
        "def root_cause_analysis(log_message):\n",
        "    \"\"\"\n",
        "    Sends the anomaly log to OpenAI LLM for a detailed RCA.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert in IT operations and log analysis. Analyze the following log anomaly and provide a detailed root cause analysis:\n",
        "\n",
        "        Log anomaly:\n",
        "        {log_message}\n",
        "\n",
        "        Root Cause Analysis:\n",
        "        \"\"\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a root cause analysis expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0.2,  # Low temperature for consistent output\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error during RCA generation: {e}\"\n",
        "\n",
        "# Gradio interface function\n",
        "def analyze_log(log_message):\n",
        "    \"\"\"\n",
        "    Classifies the log as normal or anomaly and generates RCA for anomalies.\n",
        "    \"\"\"\n",
        "    prediction = predict(log_message, model, tokenizer)\n",
        "    if prediction == 1:  # Anomalous\n",
        "        rca_result = root_cause_analysis(log_message)\n",
        "        return \"Anomaly Detected\", rca_result\n",
        "    else:\n",
        "        return \"Normal Log\", \"No RCA required for normal logs.\"\n",
        "\n",
        "# Create Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### Log Analysis and RCA\")\n",
        "\n",
        "    with gr.Row():\n",
        "        log_input = gr.Textbox(label=\"Log Message\", placeholder=\"Enter a log message\", lines=3)\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_button = gr.Button(\"Analyze Log\")\n",
        "\n",
        "    with gr.Row():\n",
        "        prediction_output = gr.Textbox(label=\"Log Classification\", interactive=False)\n",
        "        rca_output = gr.Textbox(label=\"Root Cause Analysis\", interactive=False)\n",
        "\n",
        "    # Connect function to Gradio components\n",
        "    submit_button.click(analyze_log, inputs=log_input, outputs=[prediction_output, rca_output])\n",
        "\n",
        "# Launch the app\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "OEHSQ35ncugY",
        "outputId": "b1da63e7-65c2-4580-ca44-a2774f0dd96f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://92b4e1d7e0b9f7e1db.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://92b4e1d7e0b9f7e1db.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Initialize OpenAI API\n",
        "OPENAI_API_KEY = userdata.get('OA_API')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not openai.api_key:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please configure the API key.\") # Replace with your OpenAI API key\n",
        "\n",
        "\n",
        "# Function to classify logs and perform RCA\n",
        "def process_logs(file):\n",
        "    \"\"\"\n",
        "    Processes the uploaded .log file, classifies each log as normal or anomaly using OpenAI,\n",
        "    and generates RCA for anomalies.\n",
        "    \"\"\"\n",
        "    # Read the uploaded file\n",
        "    try:\n",
        "        with open(file.name, \"r\") as f:\n",
        "            logs = f.readlines()\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading file: {e}\"\n",
        "\n",
        "    data = []\n",
        "    rca_results = []\n",
        "\n",
        "    # Loop through each log\n",
        "    for log in logs:\n",
        "        log = log.strip()  # Remove leading/trailing whitespace\n",
        "        if not log:\n",
        "            continue  # Skip empty lines\n",
        "\n",
        "        # Use OpenAI LLM to classify the log\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert in IT log analysis.\"},\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"Classify the following log as 'Normal' or 'Anomalous':\\n{log}\"\n",
        "                    },\n",
        "                ],\n",
        "                temperature=0.2,\n",
        "                max_tokens=50,\n",
        "            )\n",
        "            classification = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            classification = f\"Error: {e}\"\n",
        "\n",
        "        # Append to data\n",
        "        data.append({\"log\": log, \"classification\": classification})\n",
        "\n",
        "        # If anomalous, generate RCA\n",
        "        if classification == \"Anomalous\":\n",
        "            try:\n",
        "                rca_response = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are an expert in IT root cause analysis.\"},\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": f\"Provide a detailed root cause analysis for the following anomalous log:\\n{log}\",\n",
        "                        },\n",
        "                    ],\n",
        "                    temperature=0.2,\n",
        "                    max_tokens=300,\n",
        "                )\n",
        "                rca = rca_response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "            except Exception as e:\n",
        "                rca = f\"Error during RCA generation: {e}\"\n",
        "\n",
        "            rca_results.append({\"log\": log, \"root_cause_analysis\": rca})\n",
        "\n",
        "    # Create a DataFrame for logs and RCA results\n",
        "    df_logs = pd.DataFrame(data)\n",
        "    df_rca = pd.DataFrame(rca_results)\n",
        "\n",
        "    # Save RCA results to a CSV file\n",
        "    output_csv = \"/content/rca_results.csv\"\n",
        "    df_rca.to_csv(output_csv, index=False)\n",
        "\n",
        "    return df_logs, output_csv\n",
        "\n",
        "# Gradio UI\n",
        "def gradio_interface(file):\n",
        "    df_logs, rca_csv = process_logs(file)\n",
        "    if df_logs is None:\n",
        "        return \"Error processing file.\", None\n",
        "\n",
        "    # Display classification results\n",
        "    log_summary = df_logs.to_markdown(index=False)\n",
        "\n",
        "    # Return log summary and CSV file for download\n",
        "    return log_summary, rca_csv\n",
        "\n",
        "# Gradio app\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### Log Classification and RCA Generator\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a `.log` file containing normal and anomalous logs. \"\n",
        "        \"The app will classify the logs and generate a CSV with Root Cause Analysis for anomalies.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        log_file = gr.File(label=\"Upload .log File\")\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_button = gr.Button(\"Analyze Logs\")\n",
        "\n",
        "    with gr.Row():\n",
        "        classification_output = gr.Textbox(label=\"Log Classification Results\", lines=15, interactive=False)\n",
        "        download_link = gr.File(label=\"Download RCA CSV\")\n",
        "\n",
        "    submit_button.click(gradio_interface, inputs=log_file, outputs=[classification_output, download_link])\n",
        "\n",
        "# Launch the app\n",
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01ef45b8f7574ad48706920800423434": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03e69e2ffe5f418aaf6359bf28da48fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "094e7e2cb59b49ac99bf4b1444b437be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bc2140b288f42cc8a8ef1a65049f146": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c986cf99be440d992ff1f08437cd5de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d2135fadfe44fdebe27b0d94a26642f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_844e9a9960064aa29fa32b434bd95623",
            "placeholder": "",
            "style": "IPY_MODEL_094e7e2cb59b49ac99bf4b1444b437be",
            "value": "232k/232k[00:00&lt;00:00,3.51MB/s]"
          }
        },
        "1e86be31803744efb4baed2d0140774c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2392b709b19748bcb6f83f8b5f198be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff7bcd027aac4b889566de4ed0c1fc4a",
              "IPY_MODEL_8ca2a05d25c847fc90b1ec6ce7181bd9",
              "IPY_MODEL_5062a729b31343488f875e6eeea2cc6b"
            ],
            "layout": "IPY_MODEL_befe81fa10ae4b2a967e75aa00a36183"
          }
        },
        "2a32de61dd354dcf9ab425e3158bce68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3128f91cacce42ca8f04f28a185f0d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a22ae328d34cd590def9f2f284659a",
            "placeholder": "",
            "style": "IPY_MODEL_f50af58621514b49a4aed340c1644c7f",
            "value": "570/570[00:00&lt;00:00,54.1kB/s]"
          }
        },
        "32a2a750cda6483084c6c4efdafeb27e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "344d77509b1b43abb5d51e4470a2f2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37dc3e6a7eb9408199b4605d4ae31e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b3dc400472c48a898702c1cb4551ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4168a05fb83440efbdb665b754ca0cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a2a750cda6483084c6c4efdafeb27e",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6d2e80697f94902bf2923887deabd86",
            "value": 466062
          }
        },
        "428eee7a4e244b709dfbf5e40ffdc3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "494ff03411f4435eb9271b3fc5fa6deb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c16fdeb6b134c23977a3cb87b24e5e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c986cf99be440d992ff1f08437cd5de",
            "placeholder": "",
            "style": "IPY_MODEL_c30c9d3142d94e5bb148f70d26ae571b",
            "value": "466k/466k[00:00&lt;00:00,3.52MB/s]"
          }
        },
        "5062a729b31343488f875e6eeea2cc6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbe0e78d7a6e43ee803048847a8ddd66",
            "placeholder": "",
            "style": "IPY_MODEL_37dc3e6a7eb9408199b4605d4ae31e9d",
            "value": "440M/440M[00:02&lt;00:00,225MB/s]"
          }
        },
        "5c44ae145ad94586a0967299c9819fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5dab3148b6dc4699a1480fb79351c0dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc3697b1fe2941719d401be215b7c25b",
              "IPY_MODEL_4168a05fb83440efbdb665b754ca0cde",
              "IPY_MODEL_4c16fdeb6b134c23977a3cb87b24e5e8"
            ],
            "layout": "IPY_MODEL_a10327ff462b41b3b1ba873db3a4640e"
          }
        },
        "696497f3ea10479ba76ae7b1d7a83016": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b776697fa2e4a60ac2058cecea702c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e5af0cd54da4f6e8758e683e6a4f071",
            "placeholder": "",
            "style": "IPY_MODEL_344d77509b1b43abb5d51e4470a2f2cd",
            "value": "48.0/48.0[00:00&lt;00:00,4.25kB/s]"
          }
        },
        "6d969c4ce3294c56a873d21edeb1883a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01ef45b8f7574ad48706920800423434",
            "placeholder": "",
            "style": "IPY_MODEL_9156788c77c344758a6322209b52a51a",
            "value": "tokenizer_config.json:100%"
          }
        },
        "72918e70644a45e9abfc1dc219a2c2d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e5af0cd54da4f6e8758e683e6a4f071": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fe643368b0347c5aaf54c71e5c8c4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d969c4ce3294c56a873d21edeb1883a",
              "IPY_MODEL_91a1a4c4198249728cf22e41ffb19588",
              "IPY_MODEL_6b776697fa2e4a60ac2058cecea702c1"
            ],
            "layout": "IPY_MODEL_96f3ad2491784c8eb11ce33a64bc443a"
          }
        },
        "844e9a9960064aa29fa32b434bd95623": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85b42f3d80f04f878fd69649735631f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_696497f3ea10479ba76ae7b1d7a83016",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c44ae145ad94586a0967299c9819fb1",
            "value": 231508
          }
        },
        "8836c4e22fcc41e482f5864060b13d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e2f79be2f1540bc8124653c8203a7f8",
              "IPY_MODEL_85b42f3d80f04f878fd69649735631f4",
              "IPY_MODEL_1d2135fadfe44fdebe27b0d94a26642f"
            ],
            "layout": "IPY_MODEL_9e82cb8729be45ca9c0b2c83dea47a79"
          }
        },
        "8ca2a05d25c847fc90b1ec6ce7181bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6dcdbf8c5514e38bc320a4f066f70b1",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_924fd71d32074e90b6ed02f2160bee59",
            "value": 440449768
          }
        },
        "8e2f79be2f1540bc8124653c8203a7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bc2140b288f42cc8a8ef1a65049f146",
            "placeholder": "",
            "style": "IPY_MODEL_3b3dc400472c48a898702c1cb4551ac7",
            "value": "vocab.txt:100%"
          }
        },
        "9156788c77c344758a6322209b52a51a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91a1a4c4198249728cf22e41ffb19588": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98aaddb74e5d4bed8f4ba38592ba47d7",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03e69e2ffe5f418aaf6359bf28da48fc",
            "value": 48
          }
        },
        "924fd71d32074e90b6ed02f2160bee59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9538e28ce0bb4c09b0515fce94ba32e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e441d6f7b6d645a0bb5a588f347adbe4",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_428eee7a4e244b709dfbf5e40ffdc3dc",
            "value": 570
          }
        },
        "96f3ad2491784c8eb11ce33a64bc443a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98aaddb74e5d4bed8f4ba38592ba47d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e82cb8729be45ca9c0b2c83dea47a79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a10327ff462b41b3b1ba873db3a4640e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b96afe25cbe845d089d65df6c76d8211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc3697b1fe2941719d401be215b7c25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_494ff03411f4435eb9271b3fc5fa6deb",
            "placeholder": "",
            "style": "IPY_MODEL_72918e70644a45e9abfc1dc219a2c2d4",
            "value": "tokenizer.json:100%"
          }
        },
        "be49729795de4015bfc3c5b1b9a542af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8193f6d172949e6ba6cd807a46cb868",
            "placeholder": "",
            "style": "IPY_MODEL_cbd3ab61ad564cb7847f7095e0cde00a",
            "value": "config.json:100%"
          }
        },
        "befe81fa10ae4b2a967e75aa00a36183": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c00f523fe5614671acb278bc2b6dff79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be49729795de4015bfc3c5b1b9a542af",
              "IPY_MODEL_9538e28ce0bb4c09b0515fce94ba32e8",
              "IPY_MODEL_3128f91cacce42ca8f04f28a185f0d24"
            ],
            "layout": "IPY_MODEL_2a32de61dd354dcf9ab425e3158bce68"
          }
        },
        "c30c9d3142d94e5bb148f70d26ae571b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6d2e80697f94902bf2923887deabd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8a22ae328d34cd590def9f2f284659a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbd3ab61ad564cb7847f7095e0cde00a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbe0e78d7a6e43ee803048847a8ddd66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e441d6f7b6d645a0bb5a588f347adbe4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8193f6d172949e6ba6cd807a46cb868": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f50af58621514b49a4aed340c1644c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6dcdbf8c5514e38bc320a4f066f70b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff7bcd027aac4b889566de4ed0c1fc4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e86be31803744efb4baed2d0140774c",
            "placeholder": "",
            "style": "IPY_MODEL_b96afe25cbe845d089d65df6c76d8211",
            "value": "model.safetensors:100%"
          }
        },
        "a70dfce9e4c84b3fa2d297be347d56e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e99a3d55b7d4b65907d54b20238858b",
              "IPY_MODEL_29c7e653b5b545f291bd6b276aea5878",
              "IPY_MODEL_ecfbb65b9a4c420dac2eaf9e0603d154"
            ],
            "layout": "IPY_MODEL_aeec9f29a06947ebb6252c99413addd2"
          }
        },
        "5e99a3d55b7d4b65907d54b20238858b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7beb8ca995414a0dbf7df8688d78fe9e",
            "placeholder": "",
            "style": "IPY_MODEL_94c972afdd504316a489e03ca6470504",
            "value": "tokenizer_config.json:100%"
          }
        },
        "29c7e653b5b545f291bd6b276aea5878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7688855c5a774272af78dafe745764f7",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19140dc1521f4d2b9044c4b05815a336",
            "value": 48
          }
        },
        "ecfbb65b9a4c420dac2eaf9e0603d154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c53d5bc19be4b0d9518de41338bdeff",
            "placeholder": "",
            "style": "IPY_MODEL_b44ba3f101784e9cae124679939a7309",
            "value": "48.0/48.0[00:00&lt;00:00,521B/s]"
          }
        },
        "aeec9f29a06947ebb6252c99413addd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7beb8ca995414a0dbf7df8688d78fe9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94c972afdd504316a489e03ca6470504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7688855c5a774272af78dafe745764f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19140dc1521f4d2b9044c4b05815a336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c53d5bc19be4b0d9518de41338bdeff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b44ba3f101784e9cae124679939a7309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07e23ab3021844969e895e4822cbf303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_176bd3e0b7b6432fb1c1d7c9c8d1c629",
              "IPY_MODEL_42ea37e5520c431d8be84d67fc4af88e",
              "IPY_MODEL_c25b7ec403d647f7938b9988dc4280af"
            ],
            "layout": "IPY_MODEL_cfae9edd124c4958a598608f110a0ce5"
          }
        },
        "176bd3e0b7b6432fb1c1d7c9c8d1c629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6e36984f51a4978ad05a24d14f43a9a",
            "placeholder": "",
            "style": "IPY_MODEL_50e32412744844ee8b4bbe987fa149fc",
            "value": "vocab.txt:100%"
          }
        },
        "42ea37e5520c431d8be84d67fc4af88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff8fddc4767e4b6cb64467b988eaa11c",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f872f106d51499d9e336e2d60e4977e",
            "value": 231508
          }
        },
        "c25b7ec403d647f7938b9988dc4280af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dee307c89b0462bb21654a4f1078f23",
            "placeholder": "",
            "style": "IPY_MODEL_f829a74211f44477859c4da1bc21c14f",
            "value": "232k/232k[00:00&lt;00:00,1.88MB/s]"
          }
        },
        "cfae9edd124c4958a598608f110a0ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6e36984f51a4978ad05a24d14f43a9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50e32412744844ee8b4bbe987fa149fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff8fddc4767e4b6cb64467b988eaa11c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f872f106d51499d9e336e2d60e4977e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4dee307c89b0462bb21654a4f1078f23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f829a74211f44477859c4da1bc21c14f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15d4b22c0b13445b8febbab4d277033e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26a9cb545196440e84423d508cc24972",
              "IPY_MODEL_d200fc70cd964810bd79858d260cfe5d",
              "IPY_MODEL_4ce64bda7e1740f392c5079c4bb7f457"
            ],
            "layout": "IPY_MODEL_32856705b5824d15a29510dec45a5462"
          }
        },
        "26a9cb545196440e84423d508cc24972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81afd0e4aac34a66aa7f23cb5e3c5e54",
            "placeholder": "",
            "style": "IPY_MODEL_9b895c0dffbb4ce88022e428562a4f72",
            "value": "tokenizer.json:100%"
          }
        },
        "d200fc70cd964810bd79858d260cfe5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cfd64d76ab44dfa971a288ee90f2335",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a345f9c5c8de412ca22965bbc372c515",
            "value": 466062
          }
        },
        "4ce64bda7e1740f392c5079c4bb7f457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a550078491c4eedbae4b5da3f1e8e34",
            "placeholder": "",
            "style": "IPY_MODEL_f507e0755f9d44908f57369da8e1fdea",
            "value": "466k/466k[00:00&lt;00:00,5.78MB/s]"
          }
        },
        "32856705b5824d15a29510dec45a5462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81afd0e4aac34a66aa7f23cb5e3c5e54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b895c0dffbb4ce88022e428562a4f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cfd64d76ab44dfa971a288ee90f2335": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a345f9c5c8de412ca22965bbc372c515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a550078491c4eedbae4b5da3f1e8e34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f507e0755f9d44908f57369da8e1fdea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "840071dbe09d4dd1a94de47a92899979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3e3aebd084b4dbaaa1db4728dd95cd6",
              "IPY_MODEL_d2eefc03dd9f4aa5a444b5ac88238770",
              "IPY_MODEL_0aab503274834e4db3dd58a723c7384b"
            ],
            "layout": "IPY_MODEL_01939f3f71784da4ab4c3d770f187d9e"
          }
        },
        "b3e3aebd084b4dbaaa1db4728dd95cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d5f24bbf14d414197d0ec854d28662d",
            "placeholder": "",
            "style": "IPY_MODEL_800a7f7b60e347adbba3ee6276faf6b2",
            "value": "config.json:100%"
          }
        },
        "d2eefc03dd9f4aa5a444b5ac88238770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f780d58bab74d7d8b03722e0e1dcbd6",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e0090741fb14ef39f2438a9de08f050",
            "value": 570
          }
        },
        "0aab503274834e4db3dd58a723c7384b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_825dfbcbfa4e4226a19368faba8ddc9c",
            "placeholder": "",
            "style": "IPY_MODEL_e5dd7f0640804d1c9ba5d0d6632f4a2a",
            "value": "570/570[00:00&lt;00:00,6.59kB/s]"
          }
        },
        "01939f3f71784da4ab4c3d770f187d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d5f24bbf14d414197d0ec854d28662d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "800a7f7b60e347adbba3ee6276faf6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f780d58bab74d7d8b03722e0e1dcbd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e0090741fb14ef39f2438a9de08f050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "825dfbcbfa4e4226a19368faba8ddc9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5dd7f0640804d1c9ba5d0d6632f4a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25c55ed764ff4029bc366cf4b171ac83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ddb6bde36784389a85e0f8ebcbedaff",
              "IPY_MODEL_e0b4379952214f6daab7fc5ebe083510",
              "IPY_MODEL_a1d1588883804c38bcd33a8501748e1f"
            ],
            "layout": "IPY_MODEL_e7af2a55d33b4062b4136de70b2e388f"
          }
        },
        "3ddb6bde36784389a85e0f8ebcbedaff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea2f64a2f95d4d21bf5c436b21a2577f",
            "placeholder": "",
            "style": "IPY_MODEL_19288a32bdb34ca4bfcafbdb64b17f5b",
            "value": "model.safetensors:100%"
          }
        },
        "e0b4379952214f6daab7fc5ebe083510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_165995cc5e5746568eca7d29b11b571a",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45f58fddebc546cea89222d25bb9fc4b",
            "value": 440449768
          }
        },
        "a1d1588883804c38bcd33a8501748e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eeeb82409f6420c9e2a00a64009f009",
            "placeholder": "",
            "style": "IPY_MODEL_ccb743afdef24be7abc36e43742d3ace",
            "value": "440M/440M[00:06&lt;00:00,56.3MB/s]"
          }
        },
        "e7af2a55d33b4062b4136de70b2e388f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2f64a2f95d4d21bf5c436b21a2577f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19288a32bdb34ca4bfcafbdb64b17f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "165995cc5e5746568eca7d29b11b571a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45f58fddebc546cea89222d25bb9fc4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7eeeb82409f6420c9e2a00a64009f009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccb743afdef24be7abc36e43742d3ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}